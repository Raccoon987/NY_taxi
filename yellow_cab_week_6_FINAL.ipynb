{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import csv\n",
    "import time\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib import pyplot\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### first of all as new additional feature I am going to use dummy variable of federal US holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cal = USFederalHolidayCalendar()\n",
    "holiday = pd.get_dummies(cal.holidays(start='2014-05-01T00:00:00.000000000', end='2016-06-30T00:00:00.000000000', return_name=True))\n",
    "holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = \"./data/reg_bin_stat_2years.pkl\"\n",
    "verify_file = \"./data/verif_bin_stat_5_6_2016.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(train_file, \"rb\") as f:\n",
    "    data_2year = pickle.load(f)\n",
    "\n",
    "with open(verify_file, \"rb\") as f:\n",
    "    data_2month = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time = pd.date_range('2014 May 1 00:00:00', periods = data_2year.shape[1] - 1, freq = 'h')\n",
    "taxi = pd.DataFrame(data_2year.T[1:, :], index = time, columns=[data_2year.T[0, :].astype(int)])\n",
    "\n",
    "prediction_time = pd.date_range('2016 May 1 00:00:00', '2016 June 30 23:00:00', freq = 'h')\n",
    "taxi_prediction = pd.concat([taxi, pd.DataFrame(np.array([[None]*taxi.shape[1]]*prediction_time.shape[0]), \n",
    "                                                index = prediction_time, columns=[data_2year.T[0, :].astype(int)])], axis=0)\n",
    "\n",
    "verify_time = pd.date_range('2016 May 1 00:00:00', periods = data_2month.shape[1] - 1, freq = 'h')\n",
    "verify_taxi = pd.DataFrame(data_2month.T[1:, :], index = verify_time, columns=[data_2month.T[0, :].astype(int)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(taxi.index.values[25*24+5]) \n",
    "print(holiday.index.values[0])\n",
    "print(taxi.index.values[25*24] == holiday.index.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create mapping between number of columns and number of region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "region_map = dict(zip(list(range(102)), taxi.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_prediction.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verify_taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get weather data like in previous week "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Events:  0: nothing, 1: snow, 2: fog snow, 3: rain, 4: fog, 5: fog rain, 6: rain snow, 7: rain thunderstorm, 8: rain snow thunderstorm\", 9: thunderstorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_weather = pd.read_csv(\"./data/NY_hour_weather.csv\", usecols=[\"time\", \"daily_temp\", \"windchill\", \"humidity\", \"events\"], encoding =\"cp1252\")\n",
    "daily_weather = pd.read_csv(\"./data/NY_daily_weather.csv\", usecols=[\"daily_temp\"], encoding =\"cp1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_weather.events = hour_weather.events.replace(hour_weather.events.unique(), range(10))\n",
    "hour_weather.humidity = hour_weather.humidity.map(lambda x: x[:2])\n",
    "hour_weather.time = pd.DatetimeIndex(hour_weather.time).map(lambda x: x.replace(minute=0, second=0))\n",
    "hour_weather.drop_duplicates(subset=[\"time\"], inplace=True)\n",
    "hour_weather.set_index('time', inplace=True)\n",
    "hour_weather.shape\n",
    "\n",
    "\"\"\" from [2877:20386] \"\"\"\n",
    "test_weather = hour_weather.iloc[2877:20386]\n",
    "print(test_weather.tail())\n",
    "prediction_weather = hour_weather.iloc[20386:21850]\n",
    "print(prediction_weather.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(150,5))\n",
    "plt.plot(taxi.index.values, taxi[region_map[10]].values, color=\"red\", label='real value')\n",
    "#double click on image makes it larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### some usefull functions for feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shift(data, shift_period, column_name, external_data=pd.Series([])):\n",
    "    if external_data.empty:\n",
    "        data[column_name] = data[\"taxi_call_num\"].shift(periods=shift_period)\n",
    "    else:    \n",
    "        data[column_name] = external_data.shift(periods=shift_period) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cumsum(data, min_period, window, column_name, external_data=pd.Series([])):\n",
    "    if external_data.empty:\n",
    "        data[column_name] = data[\"taxi_call_num\"].shift(1).rolling(min_periods=min_period, window=window).sum()\n",
    "    else:\n",
    "        data[column_name] = external_data.shift(1).rolling(min_periods=min_period, window=window).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_combinations(data, fourier_shape):\n",
    "    for pair in itertools.combinations(data.columns[fourier_shape:].values, 2):\n",
    "        data[pair[0] + \"_\" + pair[1]] = data[pair[0]] * data[pair[1]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### two functions below add shifted and cumulative sum values of past taxi calls from neighbore regions, surely if they are in list of 102 considered regions; and add feature combinations - depending on \"type of interaction\" specified as one of the parameters of bivar_nonlinear function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### this two functions will mot be concidered in current notebook. use neighbore features implies sequential hourly prediction for each region since prediction of t hour for region N will require knowledge of the previously predicted taxi call value for t-1 hour of neighbore regions N-1, N+1, N+50, N-50 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_neighbore(data, reg_num, shift_dict, cumsum_dict):\n",
    "    for nghb in region_map[reg_num] + np.array([-51, -50, -49, -1, 1, 49, 50, 51]):\n",
    "        if nghb in taxi.columns.values:\n",
    "            external_data = taxi.loc[:, nghb]\n",
    "            for name, value in shift_dict:\n",
    "                shift(data, shift_period=value, column_name=(str(nghb) + name), external_data=external_data)\n",
    "            for name, value in cumsum_dict:\n",
    "                cumsum(data, min_period=value[0], window=value[1], column_name=(str(nghb) + name), external_data=external_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bivar_nonlinear(data, f, start_feature, end_feature=None):\n",
    "    for pair in itertools.combinations(data.columns[fourier_shape:].values, 2):\n",
    "        data[f.__name__ + pair[0] + \"_\" + pair[1]] = f(data[pair[0]],  data[pair[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add some new features not used in the last week's task - cu,ulative sum for last half year and and number of taxi calls 7, 28 and 365 days before + US state holidays dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_constructor(region, regres, dataframe=taxi, weather=test_weather):\n",
    "        \n",
    "    ''' regress : ((period, component_num), ...) '''\n",
    "    data = pd.DataFrame(dataframe.iloc[:, region].values, index = dataframe.index, columns=[\"taxi_call_num\"])\n",
    "    time = np.arange(0, dataframe.shape[0])\n",
    "    \n",
    "    ''' fourier components - as was shown in previous week - it was enought approx. 15 year and 70 week components '''\n",
    "    for component in regres:\n",
    "        for w in range(1, component[1]+1):\n",
    "            data[\"sin_w_%d_%d\" % (component[0], w)] = np.sin(2*np.pi*w*time/component[0]) \n",
    "            data[\"cos_w_%d_%d\" % (component[0], w)] = np.cos(2*np.pi*w*time/component[0])\n",
    "    data = data.round(3)\n",
    "    fourier_shape = data.shape[1]\n",
    "    \n",
    "    d = [ (\"d_before\", 24), (\"two_d_before\", 48), (\"three_d_before\", 72), (\"four_d_before\", 96), (\"five_d_before\", 120), \n",
    "              (\"six_d_before\", 144), (\"one_h_before\", 1), (\"two_h_before\", 2), (\"three_h_before\", 3), (\"four_h_before\", 4), \n",
    "              (\"five_h_before\", 5), (\"six_h_before\", 6), (\"seven_h_before\", 7), (\"eight_h_before\", 8), (\"nine_h_before\", 9), \n",
    "              (\"ten_h_before\", 10), (\"eleven_h_before\", 11), (\"twelve_h_before\", 12),\n",
    "              (\"year_before\", 8760), (\"four_w_before\", 672), (\"week_before\", 168)]\n",
    "\n",
    "    dd = [(\"half_d_cumsum\", (1, 12)), (\"d_cumsum\", (1, 24)), (\"week_cumsum\", (1, 168)), (\"four_w_cumsum\", (1, 168*4)),\n",
    "          (\"half_y_cumsum\", (1, 4380)), (\"year_cumsum\", (1, 8760))]\n",
    "    \n",
    "            \n",
    "    for name, value in d:\n",
    "        shift(data, shift_period=value, column_name=name)\n",
    "            \n",
    "    for name, value in dd:\n",
    "        cumsum(data, min_period=value[0], window=value[1], column_name=name)\n",
    "        \n",
    "       \n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"week_day\"] = data.index.dayofweek\n",
    "    data[\"day\"] = data.index.day\n",
    "    data[\"month\"] = data.index.month\n",
    "    \n",
    "    #add_neighbore(data, reg_num=region, shift_dict=d, cumsum_dict=dd)\n",
    "    binary_combinations(data, fourier_shape)\n",
    "    \n",
    "    data[\"weekend\"] = [1 if (date.weekday == 5 or date.weekday == 6) else 0 for date in data.index]\n",
    "    data[\"federal_holiday\"] = [1 if (np.datetime64(idx, 'D') in holiday.index) else 0 for idx in data.index]\n",
    "    #idx.astype('datetime64[D]')    \n",
    "    data = pd.concat([data, weather.iloc[:, [0, 1, 3]]], axis=1)\n",
    "    data = data.fillna(0.1)\n",
    "    \n",
    "    #make_regression(data)\n",
    "        \n",
    "    return data, fourier_shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(region, model, end_t_prediction, features):\n",
    "    feature_mixed = 31\n",
    "    \n",
    "    '''feature_constructor(region, regres, dataframe=taxi, weather=test_weather)'''\n",
    "    data, fourier_shape = feature_constructor(region=region, regres=((168, 70), (8760, 20)), dataframe=taxi_prediction, \n",
    "                               weather=pd.concat([test_weather, prediction_weather], axis=0))\n",
    "    data = data.loc[:end_t_prediction]\n",
    "    #print(\"full data shape: \", data.shape)\n",
    "    '''2016 May 1 00:00:00'''     '''2014-05-01 00:00:00'''\n",
    "    for h in pd.date_range('2016 May 1 00:00:00', end_t_prediction, freq = 'h'):\n",
    "        #print(h, end=\" \")\n",
    "        d = [ (\"d_before\", 24), (\"two_d_before\", 48), (\"three_d_before\", 72), (\"four_d_before\", 96), (\"five_d_before\", 120), \n",
    "              (\"six_d_before\", 144), (\"one_h_before\", 1), (\"two_h_before\", 2), (\"three_h_before\", 3), (\"four_h_before\", 4), \n",
    "              (\"five_h_before\", 5), (\"six_h_before\", 6), (\"seven_h_before\", 7), (\"eight_h_before\", 8), (\"nine_h_before\", 9), \n",
    "              (\"ten_h_before\", 10), (\"eleven_h_before\", 11), (\"twelve_h_before\", 12),\n",
    "              (\"year_before\", 8760), (\"four_w_before\", 672), (\"week_before\", 168)]\n",
    "        \n",
    "        dd = [(\"half_d_cumsum\", (1, 12)), (\"d_cumsum\", (1, 24)), (\"week_cumsum\", (1, 168)), (\"four_w_cumsum\", (1, 168*4)),\n",
    "              (\"half_y_cumsum\", (1, 4380)), (\"year_cumsum\", (1, 8760))]\n",
    "        \n",
    "        for name, value in d:\n",
    "            data.ix[h, name] = data.loc[h - pd.Timedelta(hours=value)]['taxi_call_num']\n",
    "                   \n",
    "        for name, value in dd:\n",
    "            data[name] = data[\"taxi_call_num\"].shift(1).rolling(min_periods=value[0], window=value[1]).sum()\n",
    "                \n",
    "        \n",
    "        ''' take only single feature columns '''\n",
    "        multicross_coll = itertools.combinations(data.columns[fourier_shape : fourier_shape + feature_mixed].values, 2)\n",
    "        for pair in multicross_coll:\n",
    "            data.ix[h, pair[0] + \"_\" + pair[1]] = data.ix[h, pair[0]] * data.ix[h, pair[1]] \n",
    "        \n",
    "        p = model.predict((data.loc[h][features].values).reshape(1, -1))\n",
    "        #p_ = prediction(model, data.loc[h][features])\n",
    "        \n",
    "        data.ix[h, 'taxi_call_num'] = p\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### try on one region using xgbregressor with default parameters and plot features histograme with their significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod = XGBRegressor()\n",
    "eleven, _ = feature_constructor(region=11, regres=((168, 70), (8760, 20)))\n",
    "mod.fit(eleven.iloc[:, 1:], eleven.taxi_call_num.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" plot importance of features with binary interactions between them for region number 1\"\"\"\n",
    "names = eleven.columns\n",
    "\n",
    "pyplot.figure(figsize=(eleven.shape[1]//4, 7))\n",
    "pyplot.bar(range(len(mod.feature_importances_)), mod.feature_importances_)\n",
    "pyplot.xticks(np.arange(names.shape[0]), names[1:], rotation=90)\n",
    "pyplot.axvline(180, color='k', linestyle='dashed', linewidth=1)\n",
    "pyplot.grid()\n",
    "pyplot.show()\n",
    "#double click on image makes it larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### select features by the r-2 value xgboost, write them into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_features(regions):\n",
    "    ''' find best features using xgboost. for each region take best features with cumsum 99%  '''\n",
    "    features_good_regions = []\n",
    "    for reg in regions:\n",
    "        print(\"REGION: \", reg, end=\" \")\n",
    "        model = XGBRegressor()\n",
    "        region, _ = feature_constructor(reg, regres=((168, 70), (8760, 20)), dataframe=taxi, weather=test_weather)\n",
    "        model.fit(region.iloc[:, 1:], region.taxi_call_num.values)\n",
    "\n",
    "        significance_list = list(filter(lambda x: x[1] > 0, \n",
    "                                        sorted(zip(region.columns[1:], model.feature_importances_), \n",
    "                                               key=lambda x: x[1], reverse=True)))\n",
    "        num_of_features = np.argwhere(np.cumsum([x[1] for x in significance_list]) > 0.95)[0][0]\n",
    "        features_good_regions.append([x[0] for x in significance_list][:num_of_features])\n",
    "    return features_good_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = {} \n",
    "for reg in range(102):\n",
    "    features[reg] = find_best_features([reg, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open(\"./data/best_features_week6.pickle\", 'wb') as handle:\n",
    "#    pickle.dump(features, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/best_features_week6.pickle\", 'rb') as handle:\n",
    "    features = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### take selected parameters for xgboost regressor found last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p={'learning_rate':0.1, 'n_estimators':400, 'max_depth':7, 'min_child_weight':5, 'gamma':0.1, 'subsample':0.9, \n",
    "            'colsample_bytree':0.6, 'objective':'reg:linear', 'scale_pos_weight':1, 'seed':27, 'reg_alpha':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_comb_bestparams = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, \n",
    "                          30, 31, 32, 33, 34, 35, 36, 37, 38, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, \n",
    "                          58, 59, 60, 61, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 81, 82, 83, 84, 88, 90, \n",
    "                          91, 92, 93, 95, 96, 97, 98, 99, 100, 101, \n",
    "                       39, 40, 52, 80, 85, 87, 89, 24, 64, 75, 10, 25, 62, 86, 94]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot prediction - may 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for reg in xgb_comb_bestparams:\n",
    "    print(\"REGION: \", reg)\n",
    "    df, _ = feature_constructor(region=reg, regres=((168, 70), (8760, 20)))\n",
    "    \n",
    "    mod = XGBRegressor(**p)\n",
    "    mod.fit(df[features[reg][0]].values, df.taxi_call_num.values)\n",
    "    print(\"XGB train r2: \", mod.score(df[features[reg][0]].values, df.taxi_call_num.values))\n",
    "    predicted_df = predict(region=reg, model=mod, end_t_prediction='2016-05-30T23:00:00.000000000', features=features[reg][0])\n",
    "    \n",
    "    print(\"train r2: \", mod.score(df[features[reg][0]].values, df.taxi_call_num.values))\n",
    "    print(\"test r2: \", r2_score(verify_taxi.loc['2016-05-01T00:00:00.000000000': '2016-05-30T23:00:00.000000000'][region_map[reg]].values, \n",
    "                                    predicted_df.loc['2016-05-01T00:00:00.000000000': '2016-05-30T23:00:00.000000000']['taxi_call_num'].values))\n",
    "\n",
    "    plt.figure(figsize=(80,10))\n",
    "    plt.plot(verify_taxi.loc['2016-05-01T00:00:00.000000000': '2016-05-30T23:00:00.000000000'].index.values, \n",
    "             verify_taxi.loc['2016-05-01T00:00:00.000000000': '2016-05-30T23:00:00.000000000'][region_map[reg]].values,\n",
    "             color=\"red\", alpha = 0.5, label='real value')\n",
    "    plt.plot(verify_taxi.loc['2016-05-01T00:00:00.000000000': '2016-05-30T23:00:00.000000000'].index.values, \n",
    "             predicted_df.loc['2016-05-01T00:00:00.000000000': '2016-05-30T23:00:00.000000000']['taxi_call_num'].values, \n",
    "             color=\"green\", alpha = 0.7, label='predicted')\n",
    "    plt.xlabel('time (hour)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try to select best parametersin another way. Previous week we choose best parameters by sequential tuning parameters one after another. Now we use BayesianOptimization package.\n",
    "##### pip install bayesian-optimization\n",
    "##### You can read about the method here: http://xgboost.readthedocs.io/en/latest/python/python_api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction results for almost all regions look better (and are better comparing r2 values) than the result of previous week. But for many regions overfitting is observed. r2 value on a train 2-year data is close to 1.0 value but on a test data set (may 2016) the result is much worse.   Tune xgboost regressor parameters for regions that showed r2 score value less than 0.8 on may 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgboostcv(max_depth,\n",
    "              learning_rate,\n",
    "              n_estimators,\n",
    "              gamma,\n",
    "              min_child_weight,\n",
    "              subsample,\n",
    "              colsample_bytree,\n",
    "              reg_alpha,\n",
    "              reg_lambda,\n",
    "              silent=True,\n",
    "              nthread=-1):\n",
    "    return cross_val_score(xgb.XGBRegressor(max_depth=int(max_depth),\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            n_estimators=int(n_estimators),\n",
    "                                            silent=silent,\n",
    "                                            nthread=nthread,\n",
    "                                            gamma=gamma,\n",
    "                                            min_child_weight=min_child_weight,\n",
    "                                            subsample=subsample,\n",
    "                                            colsample_bytree=colsample_bytree,\n",
    "                                            reg_alpha=reg_alpha,\n",
    "                                            reg_lambda=reg_lambda),\n",
    "                           x_train,\n",
    "                           y_train,\n",
    "                           'r2',\n",
    "                           cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayes_obj_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "region_tune = [10, 11, 12, 13, 14, 15, 22, 23, 24, 25, 26 ,27, 28, 38, 39, 40, 41, 42, 52, 60, 61, 64, 65, 70, 71, 72, 73, 74, \n",
    "               75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 98, 100, 101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for reg in region_tune:\n",
    "    print(reg)\n",
    "    train_df, _ = feature_constructor(region=reg, regres=((168, 70), (8760, 20)))\n",
    "    x_train, y_train = train_df.ix[:, features[reg][0]].values, train_df.taxi_call_num.values \n",
    "\n",
    "    xgboostBO = BayesianOptimization(xgboostcv, {'max_depth': (3, 14), 'learning_rate': (0.01, 0.2), 'n_estimators': (50, 1000),\n",
    "                                      'gamma': (0.01, 1.), 'min_child_weight': (1, 10), 'subsample': (0.5, 1), 'colsample_bytree' :(0.5, 1), \n",
    "                                                'reg_alpha': (1e-5, 10), 'reg_lambda': (1e-5, 10)})\n",
    "\n",
    "    xgboostBO.maximize(init_points=2, n_iter = 28)\n",
    "    bayes_obj_dict[reg] = xgboostBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open(\"H:/Yandex machine learning/finall course coursera/bayes_optimize_obj_week6_24.pickle\", 'wb') as handle:\n",
    "#    pickle.dump(bayes_obj_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open(\"F:/Yandex machine learning/finall course coursera/bayes_optimize_obj_week6_74_41_42_65.pickle\", 'rb') as handle:\n",
    "#    optimized = pickle.load(handle)\n",
    "#optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save tune results as json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for region, bayes_obj in bayes_obj_dict.items():\n",
    "    with open('./data/week_6_bayes_optimization/' + str(region) + '.json', 'w') as outfile:\n",
    "        json.dump(optimized[region].res[\"all\"], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/week_6_bayes_optimization/10.json') as outfile:\n",
    "    data = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.from_dict.html\n",
    "#http://qaru.site/questions/355841/unable-to-load-files-using-pickle-and-multipile-modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for each region sort list of different parameters (\"values\") and select largest seven elements, then fit prediction model with this parameters, make a prediction for test data - may 2016. select parameters  that showed the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_prediction = '2016-05-01T00:00:00.000000000'\n",
    "end_prediction = '2016-05-30T23:00:00.000000000'\n",
    "\n",
    "def explore_params(folder_path):\n",
    "    result = {}\n",
    "    \n",
    "    for _, _, files in os.walk(folder_path):  \n",
    "        for filename in files:\n",
    "            with open(os.path.join(folder_path, filename).replace(\"\\\\\",\"/\")) as outfile:\n",
    "                reg_num = int(filename.split('.')[0])\n",
    "                print(\"REGION: \", reg_num)\n",
    "                \n",
    "                bayes_optimize = json.load(outfile)\n",
    "                e = sorted(bayes_optimize[\"values\"])[-7]\n",
    "                \n",
    "                ''' take 5 biggest r2 values '''\n",
    "                best_param_set = np.argwhere(np.array(bayes_optimize[\"values\"]) >= e).flatten()\n",
    "                df, _ = feature_constructor(region=reg_num, regres=((168, 70), (8760, 20)))\n",
    "                \n",
    "                res = {}\n",
    "                for param_idx in best_param_set:\n",
    "                    p = bayes_optimize[\"params\"][param_idx]\n",
    "                    p = { key:(int(round(value)) if (key == 'n_estimators') or (key == 'max_depth') else value) for key, value in p.items()}\n",
    "                    \n",
    "                    print(\"cross val score r2: \", bayes_optimize[\"values\"][param_idx], end=\" \")\n",
    "                    mod = XGBRegressor(**p)\n",
    "                    mod.fit(df[features[reg_num][0]].values, df.taxi_call_num.values)\n",
    "                    r2_train =  mod.score(df[features[reg_num][0]].values, df.taxi_call_num.values)\n",
    "    \n",
    "                    print(\"train r2: \", r2_train, end=\" \")\n",
    "                    predicted_df = predict(region=reg_num, model=mod, end_t_prediction=end_prediction, features=features[reg_num][0])\n",
    "                    rs_test = r2_score(verify_taxi.loc[: end_prediction][region_map[reg_num]].values, \n",
    "                                            predicted_df.loc[start_prediction: end_prediction]['taxi_call_num'].values)\n",
    "                    print(\"test r2: \", rs_test)\n",
    "                    \n",
    "                    res[param_idx] = rs_test\n",
    "                \n",
    "                ''' get index of params element that shows best result on test set  '''\n",
    "                result[reg_num] = max(res, key=res.get)\n",
    "    return result            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "best_params = explore_params(\"./data/week_6_bayes_optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### write out best parameters (number of region: counter number of best parameters in json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' (key: value) -> (region: counter number of best parameters in json file) '''\n",
    "best_params = {64: 25, 65: 7, 11: 24, 86: 18, 70: 9, 71: 4, 72: 23, 76: 4, 74: 5, 75: 10, 12: 0, 13: 4, 14: 16, 15: 5, 80: 13, \n",
    "               81: 3, 82: 26, 90: 20, 78: 5, 22: 13, 23: 14, 88: 17, 25: 23, 26: 9, 91: 24, 93: 11, 94: 23, 100: 10, 98: 19, \n",
    "               77: 9, 101: 23, 38: 12, 39: 6, 40: 24, 41: 3, 42: 26, 10: 2, 79: 21, 52: 10, 73: 9, 60: 17, 61: 15, 87: 22, \n",
    "               84: 11, 85: 22, 87: 22, 92: 7, 24: 12, 27: 16, 28: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#best_params.update(best_params_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sorted(best_params.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### make a prediction for first week of june 2016 - for regions with initial good r2 score > 0.8 on test dataset (may 2016) will be used universal parameters set that was found in previous week, for other regions will utilize individual parameters set found by bayes optimization method and specified in json files and best_params dictionary. Selected with bayes optimization parameters helps to improve r2 score. For some regions this improvement looks impressive. \n",
    "\n",
    "##### In future it may be useful to add additional regression features - such as taxi calls from adjacent sectors or take advice indicated in the task pdf document - like number of passengers or type of payment from raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create pandas dataframe for prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" dataframe contains final prediction for all 102 regions from may 2016 - till june 2016 1-st week\"\"\"\n",
    "prediction_time = pd.date_range('2016 May 1 00:00:00', '2016 June 30 23:00:00', freq = 'h')\n",
    "full_pred_df = pd.DataFrame(np.array([[None]*taxi.shape[1]]*prediction_time.shape[0]), \n",
    "                                                index = prediction_time, columns=[data_2year.T[0, :].astype(int)])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gather all parameters from json files into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_dict = {}\n",
    "folder_path = \"./data/week_6_bayes_optimization\"\n",
    "\n",
    "for _, _, files in os.walk(folder_path):  \n",
    "    for filename in files:\n",
    "        with open(os.path.join(folder_path, filename).replace(\"\\\\\",\"/\")) as outfile:\n",
    "            reg_num = int(filename.split('.')[0])\n",
    "            values_params = json.load(outfile)\n",
    "            \n",
    "            parameters_dict[reg_num] = values_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it is possible just choose parameters that showed best result on bayes optimizer cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "parameters_dict_ = {}\n",
    "folder_path = \"./data/week_6_bayes_optimization\"\n",
    "\n",
    "for _, _, files in os.walk(folder_path):  \n",
    "    for filename in files:\n",
    "        with open(os.path.join(folder_path, filename).replace(\"\\\\\",\"/\")) as outfile:\n",
    "            reg_num = int(filename.split('.')[0])\n",
    "            values_params = json.load(outfile)\n",
    "            best_index = np.argmax(np.array(values_params[\"values\"]))\n",
    "            parameters_dict_[reg_num] = values_params[\"params\"][best_index]\n",
    "'''            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### make prediction on first week of june 2016 save it into the file and plot prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_test = '2016-05-01T00:00:00.000000000'\n",
    "end_test = '2016-05-30T23:00:00.000000000'\n",
    "start_pred = '2016-06-01T00:00:00.000000000'\n",
    "end_pred = '2016-06-07T23:00:00.000000000'\n",
    "\n",
    "for reg in range(102):\n",
    "    print(\"REGION: \", reg)\n",
    "    df, _ = feature_constructor(region=reg, regres=((168, 70), (8760, 20)))\n",
    "    \n",
    "    if reg in best_params.keys():\n",
    "        n = best_params[reg]\n",
    "        p_select = { key:(int(round(value)) if (key == 'n_estimators') or (key == 'max_depth') else value) \n",
    "             for key, value in parameters_dict[reg][\"params\"][n].items()}\n",
    "        mod = XGBRegressor(**p_select)\n",
    "    else:\n",
    "        mod = XGBRegressor(**p)\n",
    "    \n",
    "    mod.fit(df[features[reg][0]].values, df.taxi_call_num.values)\n",
    "    #print(\"XGB train r2: \", mod.score(df[features[reg][0]].values, df.taxi_call_num.values))\n",
    "    predicted_df = predict(region=reg, model=mod, end_t_prediction=end_pred, features=features[reg][0])\n",
    "    \n",
    "    print(\"train r2: \", mod.score(df[features[reg][0]].values, df.taxi_call_num.values))\n",
    "    print(\"test round to nearest int r2: \", r2_score(verify_taxi.loc[start_test: end_test][region_map[reg]].values, \n",
    "                                    np.rint(predicted_df.loc[start_test: end_test]['taxi_call_num'].values)))\n",
    "    \n",
    "    print(\"predict round to nearest int r2: \", r2_score(verify_taxi.loc[start_pred: end_pred][region_map[reg]].values, \n",
    "                                    np.rint(predicted_df.loc[start_pred: end_pred]['taxi_call_num'].values)))\n",
    "    print(\" \")\n",
    "    \n",
    "    \n",
    "    full_pred_df[region_map[reg]] = np.rint(predicted_df['taxi_call_num'])\n",
    "    \n",
    "    plt.figure(figsize=(25,10))\n",
    "    plt.plot(verify_taxi.loc[start_pred: end_pred].index.values, verify_taxi.loc[start_pred: end_pred][region_map[reg]].values,\n",
    "             color=\"red\", alpha = 0.5, label='real value')\n",
    "    plt.plot(verify_taxi.loc[start_pred: end_pred].index.values, predicted_df.loc[start_pred: end_pred]['taxi_call_num'].values, \n",
    "             color=\"green\", alpha = 0.7, label='predicted')\n",
    "    plt.xlabel('time (hour)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_pred_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mypath = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_pred_df.to_csv(mypath + \"/\" + \"final_taxi_prediction_2015-05-06.csv\", sep='\\t', columns=list(full_pred_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = pd.read_csv(mypath + \"/\" + \"final_taxi_prediction_2015-05-06.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.loc[start_test: end_pred, :].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
