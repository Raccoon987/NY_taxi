{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from math import ceil\n",
    "import random\n",
    "import itertools\n",
    "from scipy.stats import pearsonr, boxcox\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.stattools\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from scipy import spatial\n",
    "from sklearn import preprocessing\n",
    "\n",
    "''' DBSCAN, AgglomerativeClustering -probably the best results '''\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN, AgglomerativeClustering \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invboxcox(y,lmbda):\n",
    "    if lmbda == 0:\n",
    "        return(np.exp(y))\n",
    "    else:\n",
    "        return(np.exp(np.log(lmbda*y+1)/lmbda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile = './data/reg_bin_stat_2years.pkl'\n",
    "verifyfile = \"./data/verif_bin_stat_5_6_2016.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(myfile, \"rb\") as f:\n",
    "    data_2year = pickle.load(f)\n",
    "    \n",
    "with open(verifyfile, \"rb\") as f:\n",
    "    data_2month = pickle.load(f)    \n",
    "    \n",
    "#first row of transposed matrix contains region labels\n",
    "print(data_2year.T.shape)\n",
    "print(data_2month.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create two dataframes - taxi and verify_taxi for train and test. columns - regions, rows - time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verify_time = pd.date_range('2016 May 1 00:00:00', periods = data_2month.shape[1] - 1, freq = 'h')\n",
    "verify_taxi = pd.DataFrame(data_2month.T[1:, :], index = [verify_time], columns=[data_2month.T[0, :].astype(int)])\n",
    "\n",
    "time = pd.date_range('2014 May 1 00:00:00', periods = data_2year.shape[1] - 1, freq = 'h')\n",
    "taxi = pd.DataFrame(data_2year.T[1:, :], index = [time], columns=[data_2year.T[0, :].astype(int)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train data: [1 may 2014, 30 april 2016]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not to select appropriate model for each of 102 regions, task for this week suggest to cluster data. We should choose correct number of clusters. Before clusterization, apply standardization procedure (MinMaxScaler() and StandardScaler() gives here the same result) to train dataframe - subtract mean value and divide by dispersion - USE THIS STANDARTIZED DATA ONLY FOR CLUSTERIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_data = pd.DataFrame(MinMaxScaler().fit_transform(taxi), index = taxi.index, columns=taxi.columns)\n",
    "taxi_data.loc[:, taxi_data.dtypes == np.float64] = taxi_data.loc[:, taxi_data.dtypes == np.float64].apply(pd.to_numeric,downcast='float')\n",
    "taxi_data= taxi_data.round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try to choose correct number of clustersusing silhouette coefficient. Compare results for three different clusterization methods - MiniBatchKMeans - method that is appropriate for large datasets and DBSCAN, AgglomerativeClustering - two methods that usually performe well.\n",
    "##### take three month serie for clusterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Silhouette Coefficient is calculated using the mean intra-cluster distance (``a``) and the mean nearest-cluster distance (``b``) for each sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a, b)``.  To clarify, ``b`` is the distance between a sample and the nearest cluster that the sample is not a part of.\n",
    "Note that Silhouette Coefficient is only defined if number of labels is 2 <= n_labels <= n_samples - 1.\n",
    "This function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use :func: `silhouette_samples`. The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import make_blobs\n",
    "max_cluster = 10\n",
    "\n",
    "for cluster_method in (MiniBatchKMeans, AgglomerativeClustering):\n",
    "    for num_clusters in range(2, max_cluster+1):\n",
    "\n",
    "        fig, ax1 = plt.subplots(1, 1)\n",
    "        fig.set_size_inches(18, 7)\n",
    "\n",
    "        ax1.set_xlim([-0.2, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, taxi_data.iloc[:, :].T.shape[0] + (num_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        if cluster_method == AgglomerativeClustering:\n",
    "            clusterer = cluster_method(n_clusters=num_clusters)\n",
    "        elif  cluster_method == DBSCAN:  \n",
    "            clusterer = cluster_method(eps=5000, algorithm='auto')\n",
    "        else:\n",
    "            clusterer = cluster_method(n_clusters=num_clusters, random_state=10)\n",
    "        \n",
    "        cluster_labels = clusterer.fit_predict(taxi_data.iloc[:, :].T)\n",
    "        #print(cluster_labels)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples. This gives a perspective into the density\n",
    "        # and separation of the formed clusters\n",
    "        silhouette_avg = silhouette_score(taxi_data.iloc[:, :].T, cluster_labels)\n",
    "        print(\"For n_clusters =\", num_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(taxi_data.iloc[:, :].T, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(num_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.spectral(float(i) / num_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        d = {MiniBatchKMeans:\"MiniBatchKMeans\", AgglomerativeClustering:\"AgglomerativeClustering\"}\n",
    "        plt.suptitle((\"Silhouette analysis for \" + d[cluster_method] + \" clustering on sample data \"\n",
    "                      \"with n_clusters = %d\" % num_clusters),\n",
    "                     fontsize=14, fontweight='bold')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### the result look pure for all three methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it may be a right choise to use special methods developed for clustering time series f.e. -  k-Shape method  - a new fast and accurate unsupervised time-series cluster algorithm\n",
    "#### https://github.com/Mic92/kshape or https://github.com/alexminnaar/time-series-classification-and-clustering\n",
    "##### it was hard to choose correct number of clusters using Silhouette Coefficient from graphs and calculation above, so 6 clusters were choosen. (why so - look at code in supplementary materials at the end of this notebook)\n",
    "##### for futher calculation I take method describe here -  https://github.com/Mic92/kshape\n",
    "##### clusterization process takes a lot of time for long time series so it was decided to take one month long time serie - may 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from kshape.core import kshape, zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' time_series = [[1,2,3,4], [0,1,2,3], [0,1,2,3], [1,2,2,3]] '''\n",
    "cluster_num = 6\n",
    "clusters = kshape(zscore(list(taxi_data.T.values[:, :744*3]), axis=1), cluster_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(clusters)):\n",
    "    print(len(clusters[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in clusters:\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [9, 37, 48, 59, 60, 68, 70, 86] <br> [7, 11, 12, 13, 14, 15, 21, 27, 28, 31, 41, 61, 71, 72, 73, 84] <br> [74, 76, 77, 79, 80, 81, 82, 83, 85, 87, 88, 91] <br> [0, 3, 4, 5, 6, 17, 18, 19, 32, 33, 34, 35, 36, 44, 45, 95, 96, 97, 98, 99, 100, 101] <br> [8, 16, 24, 25, 26, 29, 30, 38, 39, 40, 51, 52, 62, 63, 64, 65, 75, 89, 90, 92] <br> [1, 2, 10, 20, 22, 23, 42, 43, 46, 47, 49, 50, 53, 54, 55, 56, 57, 58, 66, 67, 69, 78, 93, 94]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clusters object: [(np.array(centroid_coords), [..., idx, ...]),  (...),  (...)] - list of tuples, each tuple represent one cluter - numpy array of centroid coordinates followed by list of time serie indicies which belong to this cluster\n",
    "##### plot clusters centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_num = 6\n",
    "f, axarr = plt.subplots(int(np.ceil(plt_num / 3.0)), 3)\n",
    "f.set_size_inches(15, 10)\n",
    "for i in range(6):\n",
    "    #print(i%4, i//4)\n",
    "    axarr[i//3, i%3].plot(clusters[i][0])\n",
    "    axarr[i//3, i%3].set_title('cluster: ' + str(i))\n",
    "f.subplots_adjust(hspace=0.3)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot time serie each of 102 region to РІetermine whether they lookС‹ similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for clst_num, cluster in enumerate(clusters):\n",
    "    f, axarr = plt.subplots(int(np.ceil(len(cluster[1]) / 3.0)), 3)\n",
    "    f.set_size_inches(15, 5*int(np.ceil(len(cluster[1]) / 3.0)))\n",
    "    for idx, graph in enumerate(cluster[1]):\n",
    "        #if list(taxi_data.T.values[cluster[i], :10]) in [i[:10] for i in clust_centroids]:\n",
    "        #    axarr[i//3, i%3].plot(taxi_data.T.values[cluster[i], :744], color=\"red\") \n",
    "        axarr[idx//3, idx%3].plot(taxi_data.T.values[graph, :744])\n",
    "        axarr[idx//3, idx%3].set_title('cluster: ' + str(clst_num) + \" region index: \" + str(graph))\n",
    "    f.subplots_adjust(hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_silhouette_coeff(num_clusters, cluster_labels):\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    ax1.set_xlim([-0.3, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, taxi_data.iloc[:, :744].T.shape[0] + (num_clusters + 1) * 10])\n",
    "\n",
    "    ''' cluster_labels already exists - see  '''\n",
    "    ''' The silhouette_score gives the average value for all the samples. This gives a perspective into the density\n",
    "        and separation of the formed clusters '''\n",
    "    silhouette_avg = silhouette_score(taxi_data.iloc[:, :744].T, cluster_labels)\n",
    "    print(\"For n_clusters =\", num_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    '''' Compute the silhouette scores for each sample '''\n",
    "    sample_silhouette_values = silhouette_samples(taxi_data.iloc[:, :744].T, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    cluster_bad_elements = {}\n",
    "    for i in range(num_clusters):\n",
    "        ''' Aggregate the silhouette scores for samples belonging to cluster i, and sort them '''\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        \n",
    "        ''' select negative silhouette values for each cluster'''\n",
    "        pos = [True if ((num < 0) and (cluster_labels[idx] == i)) else False for idx, num in enumerate(sample_silhouette_values)]\n",
    "        cluster_bad_elements[i] = np.where(np.array(pos) == True)[0]\n",
    "        \n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.spectral(float(i) / num_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ''' Label the silhouette plots with their cluster numbers at the middle '''\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        ''' Compute the new y_lower for next plot '''\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    ''' The vertical line for average silhouette score of all the values '''\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    plt.show()\n",
    "    return cluster_bad_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### construct labels as array with 102 elements, each represents label of a class - e.g. [0, 0, 0, 1, 1, 2, 3, ...] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' clust_lst = [(np.array(centroid_coords), [...indexes, ...]), (...), (...)] '''\n",
    "def get_centr_label(clust_lst):\n",
    "    cluster_labels = np.array([0]*102)\n",
    "    for counter, cluster_ids in enumerate(clust_lst):\n",
    "        cluster_labels[cluster_ids[1]] = counter\n",
    "    return cluster_labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_centr_label(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouette_coeff(num_clusters=10, cluster_labels=get_centr_label(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### visual result looks much better that plot of silhouette coefficient. Maybe reason for this is that euclidean metrics was used for silhouette coefficient but k-shape method use other metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save centroids and region indexes of each cluster since the result of clustering may vary after restart (the result is not 100% reproducible)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/yellow_cab_4_week_kshape_clust.pkl', 'wb') as f:\n",
    "    pickle.dump(clusters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/yellow_cab_4_week_kshape_clust.pkl', \"rb\") as f:\n",
    "    kshape_cluster = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([clust for centroid, clust in kshape_cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster = [[9, 37, 48, 59, 60, 68, 70, 86], \n",
    "           [7, 11, 12, 13, 14, 15, 21, 27, 28, 31, 41, 61, 71, 72, 73, 84], \n",
    "           [74, 76, 77, 79, 80, 81, 82, 83, 85, 87, 88, 91], \n",
    "           [0, 3, 4, 5, 6, 17, 18, 19, 32, 33, 34, 35, 36, 44, 45, 95, 96, 97, 98, 99, 100, 101], \n",
    "           [8, 16, 24, 25, 26, 29, 30, 38, 39, 40, 51, 52, 62, 63, 64, 65, 75, 89, 90, 92], \n",
    "           [1, 2, 10, 20, 22, 23, 42, 43, 46, 47, 49, 50, 53, 54, 55, 56, 57, 58, 66, 67, 69, 78, 93, 94]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in kshape_cluster[0][1]:\n",
    "    if (kshape_cluster[0][0] == taxi_data.T.values[i, :744]).all():\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### since centriod is not one of 102 regions time serie, for each of six clusters we will find the time serie that is most correlated with the centroid and build model for this data. Six clusters - six centroids - six real time series that have the biggest correlation with centroid - six separate prediction models. We still work with first month - may 2014 - use of a two-year time sequence С€С‹ very resource demanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_corr_region(kshape_cluster):\n",
    "    best_corr_region = []\n",
    "    all_cor = []\n",
    "    for centroid, clst_idx in kshape_cluster:\n",
    "        max_corr, best_region = 0, None\n",
    "        corr_d = {}\n",
    "        for idx in clst_idx:\n",
    "            corr = pearsonr(centroid, taxi_data.T.values[idx, :744]) \n",
    "            if corr[0] > max_corr:\n",
    "                max_corr = corr[0]\n",
    "                best_region = idx\n",
    "            corr_d[idx] = corr[0]\n",
    "        all_cor.append(corr_d)    \n",
    "        print(\"correlation with centroid: \", max_corr, \" best region: \", best_region)        \n",
    "        best_corr_region.append(best_region) \n",
    "    return best_corr_region, all_cor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regions, _ = find_corr_region(kshape_cluster=kshape_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### correlation with centroid:  0.9690473254434602  best region:  48 <br> correlation with centroid:  0.8945134166302662  best region:  21 <br> correlation with centroid:  0.7650025568694776  best region:  74 <br> correlation with centroid:  0.9393446220897997  best region:  36 <br> correlation with centroid:  0.9249631166701594  best region:  30 <br> correlation with centroid:  0.916571900401084  best region:  1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### for each of this regions will be built and adjusted its own model and than this model will be applied to all regions that belong to the cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### some usefull functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' y = W.dot(X.T) '''\n",
    "def prediction(model, components, feature_num, dataframe):\n",
    "    coefs = np.array([model.params[coef] for coef in model.params.index])\n",
    "    fourier_components = np.hstack((np.array([1]*dataframe.shape[0]).reshape(dataframe.shape[0],1), \n",
    "                                    dataframe.iloc[:, 1:feature_num*components+1].values))\n",
    "    return coefs.dot(fourier_components.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_autocorr(data, lag):\n",
    "    plt.figure(figsize(15,10))\n",
    "    ax = plt.subplot(211)\n",
    "    sm.graphics.tsa.plot_acf(data.squeeze(), lags=lag, ax=ax)\n",
    "    pylab.show()\n",
    "    ax = plt.subplot(212)\n",
    "    sm.graphics.tsa.plot_pacf(data.squeeze(), lags=lag, ax=ax)\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_arima(resids, simple_dif, seson_diff, parameters_list):\n",
    "    %%time\n",
    "    residuals_results = []\n",
    "    residuals_best_aic = float(\"inf\")\n",
    "    warnings.filterwarnings('ignore')\n",
    "    resid_wrong_param = 0\n",
    "    models = {}\n",
    "\n",
    "    for counter, param in enumerate(parameters_list):\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, end=' ')\n",
    "        #some of the parameters set are incompatible\n",
    "        try:\n",
    "            model=sm.tsa.statespace.SARIMAX(resids, order=(param[0], simple_dif, param[1]), \n",
    "                                            seasonal_order=(param[2], seson_diff, param[3], 24)).fit(disp=-1)\n",
    "            models[param] = model\n",
    "        #print failure parameters and continue\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"singular matrix: \", param)\n",
    "            continue\n",
    "        except ValueError:\n",
    "            resid_wrong_param += 1\n",
    "            continue\n",
    "        aic = model.aic\n",
    "        #save best model, aic, parameters\n",
    "        if aic < residuals_best_aic:\n",
    "            residuals_best_model = model\n",
    "            residuals_best_aic = aic\n",
    "            residuals_best_param = param\n",
    "        residuals_results.append([param, model.aic])\n",
    "\n",
    "    warnings.filterwarnings('default')\n",
    "    return residuals_results, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_result(regres_res, arima_res, true_val, start_h, end_h, label):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(time[start_h:end_h], regres_res[start_h:end_h] + arima_res[start_h:end_h], alpha = 0.7, color=\"blue\", label='predicted')\n",
    "    plt.plot(time[start_h:end_h], true_val[start_h:end_h].values, alpha=0.5, label = 'real value', color=\"red\")\n",
    "    plt.title(label)\n",
    "    plt.xlabel('time (hour)')\n",
    "    plt.ylabel(\"taxi calls\")\n",
    "    plt.grid()\n",
    "    plt.legend() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' train model and plot results to compare with original time serie '''\n",
    "def all_cluster_model(regres, arima, regions, save_path=None):\n",
    "    ''' regress : ((period, component_num), ...) '''\n",
    "    ''' arima: (p,q,P,Q,d,D) '''\n",
    "    ''' regions: list of column numbers in taxi frame, coresponds to cluster '''\n",
    "    ''' if save_path - pickle file '''\n",
    "    result = []\n",
    "    for region in regions:\n",
    "        print(\"taxi column: \", region, \" region: \", taxi.columns[region], end=\" \")\n",
    "        taxi_reg = pd.DataFrame(taxi.iloc[:, region].values, index = taxi.index, columns=[\"taxi_call_num\"])\n",
    "\n",
    "        time = np.arange(0, taxi.shape[0])\n",
    "        \n",
    "        for regress_components in regres:\n",
    "            for w in range(1, regress_components[1]+1):\n",
    "                taxi_reg[\"sin_w_%d_%d\" % (regress_components[0], w)] = np.sin(2*np.pi*w*time/regress_components[0]) \n",
    "                taxi_reg[\"cos_w_%d_%d\" % (regress_components[0], w)] = np.cos(2*np.pi*w*time/regress_components[0])\n",
    "        \n",
    "        taxi_reg = taxi_reg.round(3)\n",
    "        \n",
    "        f_com = list(map(int, [0] + list(list(zip(*regres))[1])))\n",
    "        s = 'taxi_call_num ~ ' + ' + '.join([' + '.join(list(taxi_reg.columns)[1+2*sum(f_com[:i+1]): 1+2*sum(f_com[:i+2])]) \n",
    "                                                                               for i in range(len(regres))])\n",
    "        try:\n",
    "            m = smf.ols(s, data=taxi_reg)\n",
    "            model = m.fit(cov_type='HC1')\n",
    "            pred = prediction(model, components=sum(f_com), feature_num=2, dataframe=taxi_reg)\n",
    "            #residual_val = model.resid.values.astype(float32).round(3)\n",
    "            try:\n",
    "                resid_model=sm.tsa.statespace.SARIMAX(model.resid.values, \n",
    "                                                      order=(arima[0], arima[4], arima[1]), \n",
    "                                                 seasonal_order=(arima[2], arima[5], arima[3], 24)).fit(disp=-1)\n",
    "                if save_path == None:\n",
    "                    result.append(pred + resid_model.fittedvalues)\n",
    "                    #plot_result(pred, resid_model.fittedvalues, taxi_reg.taxi_call_num, 0, 745, \"may_2014\")   \n",
    "                    #plot_result(pred, resid_model.fittedvalues, taxi_reg.taxi_call_num, 3672, 4416, \"october_2014\")\n",
    "                    #plot_result(pred, resid_model.fittedvalues, taxi_reg.taxi_call_num, 13920, 14664, \"december_2015\")\n",
    "                else:\n",
    "                    #path:'H:/Yandex machine learning/finall course coursera/\n",
    "                    resid_model.save(path + \"region_%d\"%(taxi_data.columns[region]) + \"_sarimax.pkl\") \n",
    "                    with open(path + \"region_%d\"%(taxi_data.columns[region]) + \"_regression.pkl\", 'wb') as f:\n",
    "                        pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            except ValueError as e:\n",
    "                print(region, e)\n",
    "                try:\n",
    "                    log_resid, lmbda = boxcox(abs(min(model.resid.values)) + 0.1 + model.resid.values)\n",
    "                    resid_model=sm.tsa.statespace.SARIMAX(log_resid, \n",
    "                                                          order=(arima[0], arima[4], arima[1]), \n",
    "                                                     seasonal_order=(arima[2], arima[5], arima[3], 24)).fit(disp=-1)\n",
    "\n",
    "                    if save_path == None:\n",
    "                        resid_prediction = resid_model.fittedvalues\n",
    "                        resid_prediction[resid_prediction == 0] = 0.01\n",
    "                        resid_prediction = invboxcox(resid_prediction, lmbda)\n",
    "                        resid_prediction = resid_prediction - abs(min(model.resid.values)) - 0.1\n",
    "\n",
    "                        result.append(pred + resid_model.fittedvalues)\n",
    "                        #plot_result(pred, resid_prediction, taxi_reg.taxi_call_num, 0, 745, \"may_2014\")   \n",
    "                        #plot_result(pred, resid_prediction, taxi_reg.taxi_call_num, 3672, 4416, \"october_2014\")\n",
    "                        #plot_result(pred, resid_prediction, taxi_reg.taxi_call_num, 13920, 14664, \"december_2015\")\n",
    "                    else:\n",
    "                        #path:'H:/Yandex machine learning/finall course coursera/\n",
    "                        resid_model.save(path + \"region_%d\"%(taxi_data.columns[region]) + \"log_sarimax.pkl\") \n",
    "                        with open(path + \"region_%d\"%(taxi_data.columns[region]) + \"_regression.pkl\", 'wb') as f:\n",
    "                            pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                   \n",
    "                except ValueError as e:\n",
    "                    print(region, e)\n",
    "                    print(\"unfortunally boxcox does not help\")\n",
    "                        \n",
    "        #except (ValueError, LinAlgError) as e:\n",
    "        except LinAlgError as e:\n",
    "            print(region, e)\n",
    "            continue  \n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(regres, arima, region, prediction_period=('2016-05-01 00:00:00', '2016-05-31 23:00:00'), draw=True):\n",
    "    ''' regress : ((period, component_num), ...) '''\n",
    "    ''' arima: (p,q,P,Q,d,D) '''\n",
    "    ''' regions: list of column numbers in taxi frame, coresponds to cluster '''\n",
    "    ''' prediction_period: '''\n",
    "    \n",
    "    ''' regres=((168, 80), (8760, 15), (2, 23)), arima=(2, 1, 1, 1, 1, 0) '''\n",
    "    \n",
    "    print(\"taxi column: \", region, \" region: \", taxi.columns[region], end=\" \")\n",
    "    taxi_reg = pd.DataFrame(taxi.iloc[:, region].values, index = taxi.index, columns=[\"taxi_call_num\"])\n",
    "    time = np.arange(0, taxi.shape[0])\n",
    "    \n",
    "    with open(\"H:/Yandex machine learning/finall course coursera/verif_bin_stat_5_6_2016.pkl\", \"rb\") as f:\n",
    "            data_2month_verif = pickle.load(f)\n",
    "\n",
    "    index = pd.DatetimeIndex(start=prediction_period[0],end=prediction_period[1],freq='1h')\n",
    "    future_prediction = pd.DataFrame(data_2month_verif[region, 1:index.shape[0]+1], index=index, columns=[\"taxi_call_num\"])\n",
    "    future_time = np.arange(0, future_prediction.shape[0])\n",
    "    \n",
    "    \n",
    "    taxi_reg = pd.concat([taxi_reg, future_prediction], axis=0)\n",
    "      \n",
    "    for regress_components in regres:\n",
    "        for w in range(1, regress_components[1]+1):\n",
    "            taxi_reg[\"sin_w_%d_%d\" % (regress_components[0], w)] = np.sin(2*np.pi*w*np.concatenate((time, future_time))/regress_components[0]) \n",
    "            taxi_reg[\"cos_w_%d_%d\" % (regress_components[0], w)] = np.cos(2*np.pi*w*np.concatenate((time, future_time))/regress_components[0])\n",
    "    taxi_reg = taxi_reg.round(3)\n",
    "\n",
    "    f_com = list(map(int, [0] + list(list(zip(*regres))[1])))\n",
    "    s = 'taxi_call_num ~ ' + ' + '.join([' + '.join(list(taxi_reg.columns)[1+2*sum(f_com[:i+1]): 1+2*sum(f_com[:i+2])]) for i in range(len(regres))])\n",
    "    \n",
    "    \n",
    "    def draw_result():\n",
    "        plt.figure(figsize(15,7))\n",
    "        plt.plot(taxi_reg.index.values[-index.shape[0]-24*2:], taxi_reg.taxi_call_num[-index.shape[0]-24*2:], alpha = 0.5, color=\"red\", label='real value')\n",
    "        plt.plot(taxi_reg.index.values[-index.shape[0]-24*2:], (taxi_reg[\"regression_prediction\"] + taxi_reg[\"average_resid_pred\"])[-index.shape[0]-24*2:], alpha = 0.7, color=\"blue\", label='predicted')\n",
    "        plt.title(\"using past data\")\n",
    "        plt.show()  \n",
    "        plt.figure(figsize(15,7))\n",
    "        plt.plot(taxi_reg.index.values[-index.shape[0]-24*2:], taxi_reg.taxi_call_num[-index.shape[0]-24*2:], alpha = 0.5, color=\"red\", label='real value')\n",
    "        plt.plot(taxi_reg.index.values[-index.shape[0]-24*2:], (taxi_reg[\"regression_prediction\"] + taxi_reg[\"resid_prediction\"])[-index.shape[0]-24*2:], alpha = 0.7, color=\"blue\", label='predicted')\n",
    "        plt.title(\"using arima prediction\")\n",
    "        plt.show() \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        m = smf.ols(s, data=taxi_reg.iloc[:time.shape[0], :])\n",
    "        model = m.fit(cov_type='HC1')\n",
    "        pred = prediction(model, components=sum(f_com), feature_num=2, dataframe=taxi_reg)\n",
    "        \n",
    "        taxi_reg[\"regression_prediction\"] = pred\n",
    "               \n",
    "        ''' first variant '''\n",
    "        ''' ASSUME THAT THE FUTURE RESULT IN UNKNOWN, SO WE USE RESIDUALS FOR THE SAME PERIOD OF THE PAST YEAR \n",
    "            FOR ARIMA RESIDUAL PREDICTION  '''\n",
    "                \n",
    "        start_2015 = np.where(taxi_reg.index==(index[0] - pd.DateOffset(years=1)))[0][0]  \n",
    "        end_2015 = np.where(taxi_reg.index==(index[-1] - pd.DateOffset(years=1)))[0][0]\n",
    "        start_2014 = np.where(taxi_reg.index==(index[0] - pd.DateOffset(years=2)))[0][0]  \n",
    "        end_2014 = np.where(taxi_reg.index==(index[-1] - pd.DateOffset(years=2)))[0][0]\n",
    "                \n",
    "        try:\n",
    "            resid_model=sm.tsa.statespace.SARIMAX(model.resid.values, \n",
    "                                                  order=(arima[0], arima[4], arima[1]), \n",
    "                                             seasonal_order=(arima[2], arima[5], arima[3], 24)).fit(disp=-1)\n",
    "            taxi_reg[\"average_resid_pred\"] = np.concatenate((resid_model.fittedvalues, \n",
    "                                                            (resid_model.fittedvalues[start_2014:end_2014 + 1] + \n",
    "                                                             resid_model.fittedvalues[start_2015:end_2015 + 1])/2))\n",
    "            \n",
    "            taxi_reg[\"resid_prediction\"] = np.concatenate((resid_model.fittedvalues, \n",
    "                                                        resid_model.predict(start=time.shape[0], \n",
    "                                                                            end=time.shape[0] + future_time.shape[0]-1, \n",
    "                                                                            dynamic=True)))\n",
    "            if draw:\n",
    "                draw_result()\n",
    "            \n",
    "            return taxi_reg[[\"regression_prediction\", \"average_resid_pred\", \"resid_prediction\"]]\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(region, e)\n",
    "            try:\n",
    "                log_resid, lmbda = boxcox(abs(min(model.resid.values)) + 0.1 + model.resid.values)\n",
    "                resid_model=sm.tsa.statespace.SARIMAX(log_resid, \n",
    "                                                      order=(arima[0], arima[4], arima[1]), \n",
    "                                                 seasonal_order=(arima[2], arima[5], arima[3], 24)).fit(disp=-1)\n",
    "\n",
    "                resid_prediction = resid_model.fittedvalues\n",
    "                resid_prediction[resid_prediction == 0] = 0.01\n",
    "                resid_prediction = invboxcox(resid_prediction, lmbda)\n",
    "                resid_prediction = (resid_prediction - abs(min(model.resid.values)) - 0.1)\n",
    "                taxi_reg[\"average_resid_pred\"] = np.concatenate((resid_prediction, \n",
    "                                                            (resid_prediction[start_2014:end_2014 + 1] + \n",
    "                                                             resid_prediction[start_2015:end_2015 + 1])/2))\n",
    "                \n",
    "                future_resid = invboxcox(\n",
    "                         resid_model.predict(start=time.shape[0], end=time.shape[0] + future_time.shape[0]-1, dynamic=True), lmbda)                                           \n",
    "                taxi_reg[\"resid_prediction\"] = np.concatenate((resid_prediction, future_resid - abs(min(model.resid.values)) - 0.1))                                           \n",
    "\n",
    "                if draw:\n",
    "                    draw_result()\n",
    "                \n",
    "                return taxi_reg[[\"regression_prediction\", \"average_resid_pred\", \"resid_prediction\"]]\n",
    "            \n",
    "            except ValueError as e:\n",
    "                print(region, e)\n",
    "                print(\"unfortunally boxcox does not help\") \n",
    "                \n",
    "        #plot_result(pred, resid_prediction, taxi_reg.taxi_call_num, 0, 745, \"may_2016\")\n",
    "    \n",
    "    except LinAlgError as e:\n",
    "        print(region, e)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' find best fourier components and number of this components by R2 coeff and degree of correlation with typical serie lags '''\n",
    "def find_best_fourier(region, fourier_lst, ):\n",
    "    \"\"\" region - num of column \"\"\"\n",
    "    \"\"\" fourier_lst - ((number_of components, period), (number_of components, period), ) \"\"\"\n",
    "    \n",
    "    clst = pd.DataFrame(taxi.iloc[:, region].values, index = taxi.index, columns=[\"taxi_call_num\"])\n",
    "    time = np.arange(0, taxi.shape[0])\n",
    "    \n",
    "    \n",
    "    for fourier_components in fourier_lst:\n",
    "            for w in range(1, fourier_components[0]+1):\n",
    "                clst[\"sin_w_%d_%d\" % (fourier_components[1], w)] = np.sin(2*np.pi*w*time/fourier_components[1]) \n",
    "                clst[\"cos_w_%d_%d\" % (fourier_components[1], w)] = np.cos(2*np.pi*w*time/fourier_components[1])\n",
    "    \n",
    "    \n",
    "    models = []\n",
    "    f_com = list(map(int, [0] + list(list(zip(*fourier_lst))[0])))\n",
    "    s = 'taxi_call_num ~ ' + ' + '.join([' + '.join(list(clst.columns)[1+2*sum(f_com[:i+1]): 1+2*sum(f_com[:i+2])]) for i in range(len(fourier_lst)-1)])\n",
    "        \n",
    "    for W in range(1, f_com[-1]+1):\n",
    "        try:\n",
    "            current_s = s + ' + ' + ' + '.join(list(clst.columns)[1+2*sum(f_com[:len(fourier_lst)]): 1+2*sum(f_com[:len(fourier_lst)]) +2*W])\n",
    "            m = smf.ols(current_s, data=clst)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        model = m.fit(cov_type='HC1')\n",
    "        models.append(model)  \n",
    "            \n",
    "    fig1 = figure(figsize=(15,4))\n",
    " \n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    line1 = ax1.plot(range(1, f_com[-1] + 1), list(map(lambda x: x.rsquared, models)), color=\"blue\", label=\"r-square\")\n",
    "    ylabel(\"r-square\")\n",
    "\n",
    "    ax2 = fig1.add_subplot(111, sharex=ax1, frameon=False)\n",
    "    line2 = ax2.plot(range(1, f_com[-1] + 1), list(map(lambda x: x.mse_resid, models)), color=\"green\", label=\"resid\")\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.yaxis.set_label_position(\"right\")\n",
    "    ylabel(\"resid\")\n",
    "\n",
    "    legend((line1, line2), (\"1\", \"2\"))\n",
    "    legend()\n",
    "    grid()\n",
    "    show()    \n",
    "    \n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.plot(range(1, f_com[-1] + 1), [statsmodels.tsa.stattools.acf(x=model.resid, nlags=168)[168] for model in models], color=\"blue\", label='week')\n",
    "    plt.plot(range(1, f_com[-1] + 1), [statsmodels.tsa.stattools.acf(x=model.resid, nlags=365 * 24)[365 * 24] for model in models], color=\"green\", label='year')\n",
    "    plt.plot(range(1, f_com[-1] + 1), [statsmodels.tsa.stattools.acf(x=model.resid, nlags=168)[24] for model in models], color=\"red\", label='day')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"number of fourier components\")\n",
    "    plt.ylabel(\"autocorellation with 168, 24, 365*24 lags\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_real(df_1, df_2, segment, title, ylabel, size, labels):\n",
    "    plt.figure(figsize=size)\n",
    "    plt.plot(df_1[segment[0]: segment[1]], alpha = 0.7, label = labels[0])\n",
    "    plt.plot(df_2[segment[0]: segment[1]], alpha=0.5, label = labels[1], color=\"red\")\n",
    "    plt.xlabel('time (hours)')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start to create models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Cluster 0, region - column number 48 in taxi dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_clst = pd.DataFrame(taxi.iloc[:, 48].values, index = taxi.index, columns=[\"taxi_call_num\"])\n",
    "zero_clst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at entire time serie of region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (150, 15))\n",
    "axes.title.set_size(40)\n",
    "zero_clst.plot(color=\"green\", title=\"timeserie\", fontsize=25, ax = axes)\n",
    "plt.show()\n",
    "print(zero_clst.shape)\n",
    "\n",
    "#double click on image makes it bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the annual seasonality is clearly visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at first three months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_len = np.array([0, 744, 720, 744])\n",
    "\n",
    "for i, c in enumerate([\"May\", \"June\", \"July\"]):\n",
    "    zero_clst[int(sum(month_len[:i+1])) : int(sum(month_len[:i+2]))].plot(figsize=(20, 5), title=c, color=\"green\", fontsize=16, grid=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize(15,10))\n",
    "sm.tsa.seasonal_decompose(zero_clst[:2208]).plot()\n",
    "print(\"DickeyвЂ“Fuller test: p=%f\" % sm.tsa.stattools.adfuller(zero_clst.taxi_call_num[:2208].values)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### weekly seasonality is clearly visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARIMA model does not work with too big periods (more than 350)- like annual seasonality in our case also it is slow for long time series and working with large periods ARIMA is requires a lot of RAM. So we will apply ARIMA to residuals of linear regression prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARIMA use a lot of memory and works too long even with smallest of three periodocities (annual weekly daily). Each of this periodicities will be taken into account by linear regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using find_best_fourier function we may consistently choose number of each fourier components - to get best score and minimum correlation with seasonal lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT = 365 * 24\n",
    "Tt = 7 * 24\n",
    "tt = 2\n",
    "time = np.arange(0, taxi.shape[0])\n",
    "for w_week in range(1, 81):\n",
    "    zero_clst[\"sin_week_\" + str(w_week)] = np.sin(2*np.pi*w_week*time/Tt) \n",
    "    zero_clst[\"cos_week_\" + str(w_week)] = np.cos(2*np.pi*w_week*time/Tt)\n",
    "for w_year in range(1, 16):\n",
    "    zero_clst[\"sin_year_\" + str(w_year)] = np.sin(2*np.pi*w_year*time/TT) \n",
    "    zero_clst[\"cos_year_\" + str(w_year)] = np.cos(2*np.pi*w_year*time/TT) \n",
    "for w_day in range(1, 31):\n",
    "    zero_clst[\"sin_day_\" + str(w_day)] = np.sin(2*np.pi*w_day*time/tt)\n",
    "    zero_clst[\"cos_day_\" + str(w_day)] = np.cos(2*np.pi*w_day*time/tt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_clst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_fourier(48, ((80, 168), (15, 8760), (31, 2), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#models = []\n",
    "#for W in range(1, 31):\n",
    "#    m1 = smf.ols('taxi_call_num ~ ' + ' + '.join(list(zero_clst.columns)[1: 2*80+1]) + ' + ' + \n",
    "#                 ' + '.join(list(zero_clst.columns)[2*80 + 1: 2*80 + 1 + 2*15]) + ' + ' + \n",
    "#                 ' + '.join(list(zero_clst.columns)[2*80 + 1 + 2*15: 2*80 + 1 + 2*15 + 2*W]), data=zero_clst)\n",
    "#    model = m1.fit(cov_type='HC1')\n",
    "#    models.append(model)\n",
    "    \n",
    "#models[23].summary()\n",
    "# fitted.summary(); fitted.params; models[-1].rsquared; params.Intercept  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### it will be enought to choose  80 components with week period (Tt=168), 15 year components (TT=365*24) and 23 hour prediods (tt=2). Ech component consist of 2 elements sin and cos => (80+15+23)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1 = smf.ols('taxi_call_num ~ ' + ' + '.join(list(zero_clst.columns)[1: 2*80+1]) + ' + ' + \\\n",
    "                                  ' + '.join(list(zero_clst.columns)[2*80 + 1: 2*80 + 1 + 2*15]) + ' + ' + \\\n",
    "                                  ' + '.join(list(zero_clst.columns)[2*80 + 1 + 2*15: 2*80 + 1 + 2*15 + 2*23]), data=zero_clst)\n",
    "\n",
    "final_model_clust_0 = m1.fit(cov_type='HC1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = prediction(final_model_clust_0, components=118, feature_num=2, dataframe=zero_clst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_real(pred, zero_clst.taxi_call_num.values, (0, 745), \"May 2014\", 'taxi calls', (15,5), ('prediction', 'real values'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(final_model_clust_0.resid)\n",
    "plt.title(\"residual plot\")\n",
    "plt.xlabel('time (hour)')\n",
    "plt.ylabel('residuals')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### there is no annual, month seasonal periodicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at three monyj residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize(15,10))\n",
    "sm.tsa.seasonal_decompose(final_model_clust_0.resid[:2209]).plot()\n",
    "print(\"DickeyвЂ“Fuller test: p=%f\" % sm.tsa.stattools.adfuller(final_model_clust_0.resid[:4209].values)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### build ARIMA model at residual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resids = pd.DataFrame(final_model_clust_0.resid.values, index = [time], columns=[\"residuals\"])\n",
    "#resids.loc[:, resids.dtypes == np.float64] = resids.loc[:, resids.dtypes == np.float64].apply(pd.to_numeric,downcast='float')\n",
    "resids= resids.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids.residuals, lag=168)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### daily seasonality remained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resids[\"diff_1\"] = resids.residuals - resids.residuals.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids[\"diff_1\"][1:].values, lag=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Autocorrelation - РїР°СЂР°РјРµС‚СЂС‹ q, Q '''\n",
    "ps = range(0, 14)\n",
    "qs = range(0, 4)\n",
    "Ps = range(0, 2)\n",
    "Qs = range(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = itertools.product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(resids.diff_24[30].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### choose best model on first 6 month data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_results, models = find_best_arima(resids.residuals.values[0:744*6], simple_dif=1, seson_diff=0, parameters_list=parameters_list[30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(residuals_results, key=lambda tup: tup[1], reverse=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "five best results\n",
    "\n",
    "[[(13, 2, 1, 1), -62230.40738180563], [(10, 2, 1, 1), -62057.27396666515], [(6, 1, 1, 1), -61984.42501707877], [(2, 1, 1, 1), -61931.73855506932], \\n[(11, 3, 1, 1), -61876.288660259976]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### best model (13, 2, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1111 = sm.tsa.statespace.SARIMAX(resids.residuals.values, order=(13, 1, 1), \n",
    "                                        seasonal_order=(2, 0, 1, 24)).fit(disp=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(211)\n",
    "plt.plot(model_1111.resid[25:])#.plot()\n",
    "plt.ylabel(u'Residuals')\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "sm.graphics.tsa.plot_acf(model_1111.resid[25:].squeeze(), lags=168, ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot predicted and real residuals for may 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_real(model_1111.fittedvalues, resids.residuals, (0, 744), \"May 2014\", 'residuals', (15,5), \n",
    "                ('predict residuals', 'real residuals'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add ARIMA results to values fitted by linear regression - get final result. Look at fitted result for three months "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(pred, model_1111.fittedvalues, zero_clst.taxi_call_num, 0, 745, \"may_2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(pred, model_1111.fittedvalues, zero_clst.taxi_call_num, 3672, 4416, \"october_2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(pred, model_1111.fittedvalues, zero_clst.taxi_call_num, 13920, 14664, \"december_2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For all regions from cluster 0 we've got linear regression model on 80 week (t=168), 15 annual (t=365*24), 23 hour (t=2) fourier componentsС‚ + ARIMA(2, 1, 1, 1) (or (13, 2, 1, 1)) for regression residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 2) Cluster 1, region - column number 21 in taxi dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_clst = pd.DataFrame(taxi.iloc[:, 21].values, index = taxi.index, columns=[\"taxi_call_num\"])\n",
    "one_clst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at entire time serie of region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (150,15))\n",
    "axes.title.set_size(40)\n",
    "one_clst.plot(color=\"green\", title=\"timeserie\", fontsize=25, ax = axes)\n",
    "plt.show()\n",
    "print(one_clst.shape)\n",
    "\n",
    "#double click on image makes it bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at first three months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_len = np.array([0, 744, 720, 744])\n",
    "\n",
    "for i, c in enumerate([\"May\", \"June\", \"July\"]):\n",
    "    one_clst[int(sum(month_len[:i+1])) : int(sum(month_len[:i+2]))].plot(figsize=(20, 5), title=c, color=\"green\", fontsize=16, grid=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize(15,10))\n",
    "sm.tsa.seasonal_decompose(one_clst[:2208]).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### do all the same as in previous episode with cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT = 365 * 24\n",
    "Tt = 7 * 24\n",
    "tt = 1\n",
    "time = np.arange(0, taxi.shape[0])\n",
    "for w_week in range(1, 71):\n",
    "    one_clst[\"sin_week_\" + str(w_week)] = np.sin(2*np.pi*w_week*time/Tt) \n",
    "    one_clst[\"cos_week_\" + str(w_week)] = np.cos(2*np.pi*w_week*time/Tt)\n",
    "for w_day in range(1, 13):\n",
    "    one_clst[\"sin_day_\" + str(w_day)] = np.sin(2*np.pi*w_day*time/tt)\n",
    "    one_clst[\"cos_day_\" + str(w_day)] = np.cos(2*np.pi*w_day*time/tt) \n",
    "for w_year in range(1, 16):\n",
    "    one_clst[\"sin_year_\" + str(w_year)] = np.sin(2*np.pi*w_year*time/TT) \n",
    "    one_clst[\"cos_year_\" + str(w_year)] = np.cos(2*np.pi*w_year*time/TT)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_fourier(21, ((70, 168), (15, 8760), (30, 1)))\n",
    "\"\"\" region - num of column \"\"\"\n",
    "\"\"\" fourier_lst - ((number_of components, period), (number_of components, period), ) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it will be enought to choose 70 components with week period (Tt=168), 12 year components (TT=36524) and 15 hour prediods (tt=1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m2 = smf.ols('taxi_call_num ~ ' + ' + '.join(list(one_clst.columns)[1: 2*70+1]) + ' + ' + \n",
    "                                  ' + '.join(list(one_clst.columns)[2*70 + 1: 2*70 + 1 + 2*12]) \n",
    "                                + ' + ' +' + '.join(list(one_clst.columns)[2*70 + 1 + 2*12: 2*70 + 1 + 2*12 + 2*15]), data=one_clst)\n",
    "model = m2.fit(cov_type='HC1')\n",
    "final_model_clust_1 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(final_model_clust_1.resid)\n",
    "plt.title(\"residual plot\")\n",
    "plt.xlabel('time (hour)')\n",
    "plt.ylabel('residuals')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids1 = pd.DataFrame(final_model_clust_1.resid.values, index = [time], columns=[\"residuals\"]).round(2)\n",
    "resids1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids1.residuals, lag=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resids1[\"diff_1\"] = resids1.residuals - resids1.residuals.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids1[\"diff_1\"][1:], lag=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Autocorrelation - РїР°СЂР°РјРµС‚СЂС‹ q, Q '''\n",
    "ps = range(0, 8)\n",
    "qs = range(0, 14)\n",
    "Ps = range(0, 2)\n",
    "Qs = range(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = itertools.product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_results, models = find_best_arima(resids1.residuals.values[0:744*6], simple_dif=1, seson_diff=0, parameters_list=parameters_list[300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(residuals_results, key=lambda tup: tup[1], reverse=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best models\n",
    "\n",
    "[[(5, 1, 1, 1), 47675.95240058894], [(6, 1, 1, 1), 47677.91827414791], [(7, 1, 1, 1), 47681.338857963274], [(6, 12, 1, 1), 47692.614959403974], [(6, 2, 1, 1), 47693.0237124409]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### best model (5, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = sm.tsa.statespace.SARIMAX(resids1.residuals.values, order=(5, 1, 1), \n",
    "                                        seasonal_order=(1, 0, 1, 24)).fit(disp=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(211)\n",
    "plt.plot(best_model.resid[1:])#.plot()\n",
    "plt.ylabel(u'Residuals')\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "sm.graphics.tsa.plot_acf(best_model.resid[1:].squeeze(), lags=168, ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot predicted and real residuals for may 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_real(best_model.fittedvalues, resids.residuals, (0, 744), \"May 2014\", 'residuals', (15,5), \n",
    "                ('predict residuals', 'real residuals'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add ARIMA results to values fitted by linear regression - get final result. Look at fitted result for two months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(prediction(final_model_clust_1, components=97, feature_num=2, dataframe=one_clst), \n",
    "            best_model.fittedvalues, one_clst.taxi_call_num, 0, 745, \"may_2014\")\n",
    "plot_result(prediction(final_model_clust_1, components=97, feature_num=2, dataframe=one_clst), \n",
    "            best_model.fittedvalues, one_clst.taxi_call_num, 13920, 14664, \"december_2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For all regions from cluster 1 we've got linear regression model on 70 week (t=168), 15 annual (t=365*24), 12 hour (t=1) fourier componentsС‚ + ARIMA(5, 1, 1, 1) for regression residuals. Use this model to fit all cluster regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Cluster 2, region - column number 74 in taxi dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_clst = pd.DataFrame(taxi.iloc[:, 74].values, index = taxi.index, columns=[\"taxi_call_num\"])\n",
    "two_clst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (150,15))\n",
    "axes.title.set_size(40)\n",
    "two_clst.plot(color=\"green\", title=\"timeserie\", fontsize=25, ax = axes)\n",
    "plt.show()\n",
    "print(two_clst.shape)\n",
    "\n",
    "#double click on image makes it bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the annual seasonality is clearly visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at first three months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_len = np.array([0, 744, 720, 744])\n",
    "\n",
    "for i, c in enumerate([\"May\", \"June\", \"July\"]):\n",
    "    two_clst[int(sum(month_len[:i+1])) : int(sum(month_len[:i+2]))].plot(figsize=(20, 5), title=c, color=\"green\", fontsize=16, grid=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize(15,10))\n",
    "sm.tsa.seasonal_decompose(pd.Series(two_clst.taxi_call_num[:2208])).plot()\n",
    "print(\"DickeyвЂ“Fuller test: p=%f\" % sm.tsa.stattools.adfuller(two_clst.taxi_call_num[:2208].values)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### weekly seasonality is clearly visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using find_best_fourier function we may consistently choose number of each fourier components - to get best score and minimum correlation with seasonal lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_fourier(74, ((70, 168), (20, 8760), (30, 1), ))\n",
    "\"\"\" region - num of column \"\"\"\n",
    "\"\"\" fourier_lst - ((number_of components, period), (number_of components, period), ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT = 365 * 24\n",
    "Tt = 7 * 24\n",
    "tt = 1\n",
    "time = np.arange(0, taxi.shape[0])\n",
    "for w_week in range(1, 71):\n",
    "    two_clst[\"sin_week_\" + str(w_week)] = np.sin(2*np.pi*w_week*time/Tt) \n",
    "    two_clst[\"cos_week_\" + str(w_week)] = np.cos(2*np.pi*w_week*time/Tt)\n",
    "for w_day in range(1, 13):\n",
    "    two_clst[\"sin_day_\" + str(w_day)] = np.sin(2*np.pi*w_day*time/tt)\n",
    "    two_clst[\"cos_day_\" + str(w_day)] = np.cos(2*np.pi*w_day*time/tt) \n",
    "for w_year in range(1, 21):\n",
    "    two_clst[\"sin_year_\" + str(w_year)] = np.sin(2*np.pi*w_year*time/TT) \n",
    "    two_clst[\"cos_year_\" + str(w_year)] = np.cos(2*np.pi*w_year*time/TT)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it will be enought to choose 70 components with week period (Tt=168), 20 year components (TT=36524) and 12 hour prediods (tt=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m3 = smf.ols('taxi_call_num ~ ' + ' + '.join(list(two_clst.columns)[1: 2*70+1]) + ' + ' + \n",
    "                                  ' + '.join(list(two_clst.columns)[2*70 + 1: 2*70 + 1 + 2*12]) \n",
    "                                + ' + ' +' + '.join(list(two_clst.columns)[2*70 + 1 + 2*12: 2*70 + 1 + 2*12 + 2*20]), data=two_clst)\n",
    "final_model_clust_2 = m3.fit(cov_type='HC1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(final_model_clust_2.resid)\n",
    "plt.title(\"residual plot\")\n",
    "plt.xlabel('time (hour)')\n",
    "plt.ylabel('residuals')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### there is no annual, month seasonal periodicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at three month residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids2 = pd.DataFrame(final_model_clust_2.resid.values, index = [time], columns=[\"residuals\"]).round(2)\n",
    "resids2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids2.residuals, lag=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resids2[\"diff_1\"] = resids2.residuals - resids2.residuals.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids2[\"diff_1\"][1:], lag=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Autocorrelation - РїР°СЂР°РјРµС‚СЂС‹ q, Q '''\n",
    "ps = range(0, 18)\n",
    "qs = range(0, 4)\n",
    "Ps = range(0, 2)\n",
    "Qs = range(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = itertools.product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_results, models = find_best_arima(resids2.residuals.values[0:744*6], simple_dif=1, seson_diff=0, \n",
    "                                            parameters_list=parameters_list[264:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(residuals_results, key=lambda tup: tup[1], reverse=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best models\n",
    "\n",
    "[[(14, 1, 1, 1), 26649.113279118814], [(17, 1, 1, 1), 26652.25337989754], [(15, 2, 1, 1), 26655.680252936225], [(16, 1, 1, 1), 26655.999645383225], [(11, 1, 1, 1), 26656.231480555634], [(9, 1, 1, 1), 26657.434251443105], [(12, 2, 1, 1), 26658.540766088317], [(11, 3, 1, 1), 26659.657781788857], [(12, 1, 1, 1), 26659.975405869918], [(9, 2, 1, 1), 26660.11414850546]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = sm.tsa.statespace.SARIMAX(resids2.residuals.values, order=(14, 1, 1), \n",
    "                                        seasonal_order=(1, 0, 1, 24)).fit(disp=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(211)\n",
    "plt.plot(best_model.resid[1:])#.plot()\n",
    "plt.ylabel(u'Residuals')\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "sm.graphics.tsa.plot_acf(best_model.resid[1:].squeeze(), lags=168, ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot predicted and real residuals for may 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_real(best_model.fittedvalues, resids2.residuals, (0, 744), \"May 2014\", 'residuals', (15,5), \n",
    "                ('predict residuals', 'real residuals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(prediction(final_model_clust_2, components=102, feature_num=2, dataframe=two_clst), \n",
    "            best_model.fittedvalues, two_clst.taxi_call_num, 0, 745, \"may_2014\")\n",
    "plot_result(prediction(final_model_clust_2, components=102, feature_num=2, dataframe=two_clst), \n",
    "            best_model.fittedvalues, two_clst.taxi_call_num, 13920, 14664, \"december_2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For all regions from cluster 2 we've got linear regression model on 70 week (t=168), 20 annual (t=365*24), 12 hour (t=1) fourier componentsС‚ + ARIMA(14, 1, 1, 1) for regression residuals. Use this model to fit all cluster regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Cluster 3, region - column number 36 in taxi dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_clst = pd.DataFrame(taxi.iloc[:, 36].values, index = taxi.index, columns=[\"taxi_call_num\"])\n",
    "three_clst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at entire time serie of region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (150,15))\n",
    "axes.title.set_size(40)\n",
    "three_clst.plot(color=\"green\", title=\"timeserie\", fontsize=25, ax = axes)\n",
    "plt.show()\n",
    "print(three_clst.shape)\n",
    "\n",
    "#double click on image makes it bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the annual seasonality is clearly visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at first three months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_len = np.array([0, 744, 720, 744])\n",
    "\n",
    "for i, c in enumerate([\"May\", \"June\", \"July\"]):\n",
    "    three_clst[int(sum(month_len[:i+1])) : int(sum(month_len[:i+2]))].plot(figsize=(20, 5), title=c, color=\"green\", fontsize=16, grid=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize(15,10))\n",
    "sm.tsa.seasonal_decompose(three_clst.iloc[2208*0:2208*1, 0]).plot()\n",
    "print(\"DickeyвЂ“Fuller test: p=%f\" % sm.tsa.stattools.adfuller(three_clst.taxi_call_num[2208*0:2208*1].values)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT = 365 * 24\n",
    "Tt = 7 * 24\n",
    "tt = 11\n",
    "time = np.arange(0, taxi.shape[0])\n",
    "for w_week in range(1, 61):\n",
    "    three_clst[\"sin_week_\" + str(w_week)] = np.sin(2*np.pi*w_week*time/Tt) \n",
    "    three_clst[\"cos_week_\" + str(w_week)] = np.cos(2*np.pi*w_week*time/Tt)\n",
    "for w_year in range(1, 16):\n",
    "    three_clst[\"sin_year_\" + str(w_year)] = np.sin(2*np.pi*w_year*time/TT) \n",
    "    three_clst[\"cos_year_\" + str(w_year)] = np.cos(2*np.pi*w_year*time/TT)\n",
    "for w_day in range(1, 13):\n",
    "    three_clst[\"sin_day_\" + str(w_day)] = np.sin(2*np.pi*w_day*time/tt)\n",
    "    three_clst[\"cos_day_\" + str(w_day)] = np.cos(2*np.pi*w_day*time/tt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_clst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using find_best_fourier function we may consistently choose number of each fourier components - to get best score and minimum correlation with seasonal lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for W in range(1, 13):\n",
    "    m4 = smf.ols('taxi_call_num ~ ' + ' + '.join(list(three_clst.columns)[1: 2*60+1]) + ' + ' + \n",
    "                 ' + '.join(list(three_clst.columns)[2*60 + 1: 2*60 + 1 + 2*15]) + \n",
    "                 ' + ' + ' + '.join(list(three_clst.columns)[2*60 + 1 + 2*15: 2*60 + 1 + 2*15 + 2*W]), data=three_clst)\n",
    "    model = m4.fit(cov_type='HC1')\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_fourier(36, ((60, 168), (15, 8760), (30, 11), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it will be enought to choose 60 components with week period (Tt=168), 15 year components (TT=365*24) and 12 hour prediods (tt=11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m4 = smf.ols('taxi_call_num ~ ' + ' + '.join(list(three_clst.columns)[1: 2*60+1]) + ' + ' + \n",
    "                                  ' + '.join(list(three_clst.columns)[2*60 + 1: 2*60 + 1 + 2*15]) \n",
    "                                + ' + ' +' + '.join(list(three_clst.columns)[2*60 + 1 + 2*15: 2*60 + 1 + 2*15 + 2*12]), data=three_clst)\n",
    "model = m4.fit(cov_type='HC1')\n",
    "final_model_clust_3 = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(final_model_clust_3.resid)\n",
    "plt.title(\"residual plot\")\n",
    "plt.xlabel('time (hour)')\n",
    "plt.ylabel('residuals')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids3 = pd.DataFrame(final_model_clust_3.resid.values, index = [time], columns=[\"residuals\"]).round(2)\n",
    "resids3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids3.residuals, lag=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resids3[\"diff_1\"] = resids3.residuals - resids3.residuals.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids3[\"diff_1\"][1:], lag=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Autocorrelation - РїР°СЂР°РјРµС‚СЂС‹ q, Q '''\n",
    "ps = range(0, 5)\n",
    "qs = range(0, 5)\n",
    "Ps = range(0, 2)\n",
    "Qs = range(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = itertools.product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_results, models = find_best_arima(resids3.residuals.values[0:744*6], simple_dif=1, seson_diff=0, \n",
    "                                            parameters_list=parameters_list[80:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(residuals_results, key=lambda tup: tup[1], reverse=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "best models\n",
    "\n",
    "[[(3, 1, 1, 1), 46776.46106883851], [(2, 1, 1, 1), 46779.42063216587], [(1, 1, 1, 1), 46780.752699463446], [(1, 2, 1, 1), 46781.47510706796], [(1, 4, 1, 1), 46790.923233770605]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### best model (3, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = sm.tsa.statespace.SARIMAX(resids3.residuals.values, order=(3, 1, 1), \n",
    "                                        seasonal_order=(1, 0, 1, 24)).fit(disp=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(211)\n",
    "plt.plot(best_model.resid[1:])\n",
    "plt.ylabel(u'Residuals')\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "sm.graphics.tsa.plot_acf(best_model.resid[1:].squeeze(), lags=168, ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot predicted and real residuals for may 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_real(best_model.fittedvalues, resids3.residuals, (0, 744), \"May 2014\", 'residuals', (15,5), \n",
    "                ('predict residuals', 'real residuals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(prediction(final_model_clust_3, components=87, feature_num=2, dataframe=three_clst), \n",
    "            best_model.fittedvalues, three_clst.taxi_call_num, 0, 745, \"may_2014\")\n",
    "plot_result(prediction(final_model_clust_3, components=87, feature_num=2, dataframe=three_clst), \n",
    "            best_model.fittedvalues, three_clst.taxi_call_num, 13920, 14664, \"december_2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### For all regions from cluster 3 we've got linear regression model on 60 week (t=168), 15 annual (t=365*24), 12 hour (t=11) fourier componentsС‚ + ARIMA(3, 1, 1, 1) for regression residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Cluster 4, region - column number 30 in taxi dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_clst = pd.DataFrame(taxi.iloc[:, 30].values, index = taxi.index, columns=[\"taxi_call_num\"])\n",
    "four_clst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (150,15))\n",
    "axes.title.set_size(40)\n",
    "four_clst.plot(color=\"green\", title=\"timeserie\", fontsize=25, ax = axes)\n",
    "plt.show()\n",
    "print(four_clst.shape)\n",
    "\n",
    "#double click on image makes it bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "month_len = np.array([0, 744, 720, 744])\n",
    "\n",
    "for i, c in enumerate([\"May\", \"June\", \"July\"]):\n",
    "    four_clst[int(sum(month_len[:i+1])) : int(sum(month_len[:i+2]))].plot(figsize=(20, 5), title=c, color=\"green\", fontsize=16, \n",
    "                                                                          grid=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize(15,10))\n",
    "sm.tsa.seasonal_decompose(four_clst[:2208]).plot()\n",
    "print(\"DickeyвЂ“Fuller test: p=%f\" % sm.tsa.stattools.adfuller(four_clst.taxi_call_num[:2208].values)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT = 365 * 24\n",
    "Tt = 7 * 24\n",
    "tt = 1\n",
    "time = np.arange(0, taxi.shape[0])\n",
    "for w_week in range(1, 61):\n",
    "    four_clst[\"sin_week_\" + str(w_week)] = np.sin(2*np.pi*w_week*time/Tt) \n",
    "    four_clst[\"cos_week_\" + str(w_week)] = np.cos(2*np.pi*w_week*time/Tt)\n",
    "for w_year in range(1, 21):\n",
    "    four_clst[\"sin_year_\" + str(w_year)] = np.sin(2*np.pi*w_year*time/TT) \n",
    "    four_clst[\"cos_year_\" + str(w_year)] = np.cos(2*np.pi*w_year*time/TT)\n",
    "for w_day in range(1, 31):\n",
    "    four_clst[\"sin_day_\" + str(w_day)] = np.sin(2*np.pi*w_day*time/tt)\n",
    "    four_clst[\"cos_day_\" + str(w_day)] = np.cos(2*np.pi*w_day*time/tt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using find_best_fourier function we may consistently choose number of each fourier components - to get best score and minimum correlation with seasonal lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_fourier(30, ((60, 168), (20, 8760), (60, 1), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for W in range(1, 31):\n",
    "    m5 = smf.ols('taxi_call_num ~ ' + ' + '.join(list(four_clst.columns)[1: 2*60+1]) + ' + ' + \n",
    "                 ' + '.join(list(four_clst.columns)[2*60 + 1: 2*60 + 1 + 2*20]) + \n",
    "                 ' + ' + ' + '.join(list(four_clst.columns)[2*60 + 1 + 2*20: 2*60 + 1 + 2*20 + 2*W]), data=four_clst)\n",
    "    model = m5.fit(cov_type='HC1')\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 60 components with week period (Tt=168), 20 year components (TT=36524) and 20 hour prediods (tt=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m5 = smf.ols('taxi_call_num ~ ' + ' + '.join(list(four_clst.columns)[1: 2*60+1]) + ' + ' + \n",
    "                                  ' + '.join(list(four_clst.columns)[2*60 + 1: 2*60 + 1 + 2*20]) \n",
    "                                + ' + ' +' + '.join(list(four_clst.columns)[2*60 + 1 + 2*20: 2*60 + 1 + 2*20 + 2*20])\n",
    "                                , data=four_clst)\n",
    "                                #+ ' + november_anom', data=four_clst)\n",
    "\n",
    "final_model_clust_4 = m5.fit(cov_type='HC1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 60 РЅРµРґРµР»СЊРЅС‹С…, 20 РіРѕРґРѕРІС‹С… Рё 20 С‡Р°СЃРѕРІС‹С… РєРѕРјРїРѕРЅРµРЅС‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(final_model_clust_4.resid)\n",
    "plt.title(\"residual plot\")\n",
    "plt.xlabel('time (hour)')\n",
    "plt.ylabel('residuals')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids4 = pd.DataFrame(final_model_clust_4.resid.values, index = [time], columns=[\"residuals\"]).round(2)\n",
    "resids4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids4.residuals, lag=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resids4[\"diff_1\"] = resids4.residuals - resids4.residuals.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids4[\"diff_1\"][1:], lag=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Autocorrelation - РїР°СЂР°РјРµС‚СЂС‹ q, Q '''\n",
    "ps = range(0, 15)\n",
    "qs = range(0, 8)\n",
    "Ps = range(0, 2)\n",
    "Qs = range(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = itertools.product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_results, models = find_best_arima(resids4.residuals.values[0:744*6], simple_dif=1, seson_diff=0, \n",
    "                                            parameters_list=parameters_list[400:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(residuals_results, key=lambda tup: tup[1], reverse=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best models\n",
    "\n",
    "[[(9, 2, 1, 1), 47741.49595955429], [(8, 2, 1, 1), 47742.793918648335], [(14, 3, 1, 1), 47743.70836576415], [(6, 2, 1, 1), 47744.615492413956], [(4, 3, 1, 1), 47744.76752001122]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### best model: (9, 2, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = sm.tsa.statespace.SARIMAX(resids4.residuals.values, order=(9, 1, 2), \n",
    "                                        seasonal_order=(1, 0, 1, 24)).fit(disp=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.columns[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(211)\n",
    "plt.plot(best_model.resid[1:])#.plot()\n",
    "plt.ylabel(u'Residuals')\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "sm.graphics.tsa.plot_acf(best_model.resid[1:].squeeze(), lags=168, ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot predicted and real residuals for may 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_real(best_model.fittedvalues, resids4.residuals, (0, 744), \"May 2014\", 'residuals', (15,5), \n",
    "                ('predict residuals', 'real residuals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(prediction(final_model_clust_4, components=100, feature_num=2, dataframe=four_clst), \n",
    "            best_model.fittedvalues, four_clst.taxi_call_num, 0, 745, \"may_2014\")\n",
    "plot_result(prediction(final_model_clust_4, components=100, feature_num=2, dataframe=four_clst), \n",
    "            best_model.fittedvalues, four_clst.taxi_call_num, 13920, 14664, \"december_2015\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### For all regions from cluster 4 we've got linear regression model on 60 week (t=168), 20 annual (t=365*24), 20 hour (t=1) fourier componentsС‚ + ARIMA(9, 2, 1, 1) for regression residuals. Use this model to fit all cluster regions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Cluster 5, region - column number 1 in taxi dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_clst = pd.DataFrame(taxi.iloc[:, 1].values, index = taxi.index, columns=[\"taxi_call_num\"])\n",
    "five_clst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (150,15))\n",
    "axes.title.set_size(40)\n",
    "five_clst.plot(color=\"green\", title=\"timeserie\", fontsize=25, ax = axes)\n",
    "plt.show()\n",
    "print(five_clst.shape)\n",
    "\n",
    "#double click on image makes it bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_len = np.array([0, 744, 720, 744])\n",
    "\n",
    "for i, c in enumerate([\"May\", \"June\", \"July\"]):\n",
    "    five_clst[int(sum(month_len[:i+1])) : int(sum(month_len[:i+2]))].plot(figsize=(20, 5), title=c, color=\"green\", fontsize=16, grid=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize(15,10))\n",
    "sm.tsa.seasonal_decompose(five_clst[:2208]).plot()\n",
    "print(\"DickeyвЂ“Fuller test: p=%f\" % sm.tsa.stattools.adfuller(five_clst.taxi_call_num[:2208].values)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT = 365 * 24\n",
    "Tt = 7 * 24\n",
    "tt = 1\n",
    "time = np.arange(0, taxi.shape[0])\n",
    "for w_week in range(1, 61):\n",
    "    five_clst[\"sin_week_\" + str(w_week)] = np.sin(2*np.pi*w_week*time/Tt) \n",
    "    five_clst[\"cos_week_\" + str(w_week)] = np.cos(2*np.pi*w_week*time/Tt)\n",
    "for w_day in range(1, 31):\n",
    "    five_clst[\"sin_day_\" + str(w_day)] = np.sin(2*np.pi*w_day*time/tt)\n",
    "    five_clst[\"cos_day_\" + str(w_day)] = np.cos(2*np.pi*w_day*time/tt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_clst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using find_best_fourier function we may consistently choose number of each fourier components - to get best score and minimum correlation with seasonal lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_fourier(1, ((60, 168), (60, 1), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 60 components with week period (Tt=168) and 12 hour prediods (tt=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m6 = smf.ols('taxi_call_num ~ ' + ' + '.join(list(five_clst.columns)[1: 2*60+1]) + ' + ' + \n",
    "                 ' + '.join(list(five_clst.columns)[2*60 + 1: 2*60 + 1 + 2*12]), data=five_clst)\n",
    "\n",
    "final_model_clust_5 = m6.fit(cov_type='HC1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(final_model_clust_5.resid)\n",
    "plt.title(\"May-June-July_2014\")\n",
    "plt.xlabel('time (hour)')\n",
    "plt.ylabel('residuals')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "resids5 = pd.DataFrame(final_model_clust_5.resid.values, index = [time], columns=[\"residuals\"]).round(2)\n",
    "resids5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids5.residuals, lag=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resids5[\"diff_1\"] = resids5.residuals - resids5.residuals.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autocorr(resids5[\"diff_1\"][1:], lag=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Autocorrelation - РїР°СЂР°РјРµС‚СЂС‹ q, Q '''\n",
    "ps = range(0, 14)\n",
    "qs = range(0, 2)\n",
    "Ps = range(0, 2)\n",
    "Qs = range(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = itertools.product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_results, models = find_best_arima(resids5.residuals.values[0:744*6], simple_dif=1, seson_diff=0, \n",
    "                                            parameters_list=parameters_list[80:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(residuals_results, key=lambda tup: tup[1], reverse=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best models\n",
    "\n",
    "[[(10, 1, 1, 1), 38552.39576565322], [(11, 1, 1, 1), 38552.702862086866], [(5, 1, 1, 1), 38554.870367152005], [(6, 1, 1, 1), 38555.352233191625], [(13, 1, 1, 1), 38555.85244020156]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### best model: (10, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = sm.tsa.statespace.SARIMAX(resids5.residuals.values, order=(11, 1, 1), \n",
    "                                        seasonal_order=(1, 0, 1, 24)).fit(disp=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(211)\n",
    "plt.plot(best_model.resid[1:])#.plot()\n",
    "plt.ylabel(u'Residuals')\n",
    "\n",
    "ax = plt.subplot(212)\n",
    "sm.graphics.tsa.plot_acf(best_model.resid[1:].squeeze(), lags=168, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot predicted and real residuals for may 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_real(best_model.fittedvalues, resids5.residuals, (0, 744), \"May 2014\", 'residuals', (15,5), \n",
    "                ('predict residuals', 'real residuals'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(prediction(final_model_clust_5, components=72, feature_num=2, dataframe=five_clst), \n",
    "            best_model.fittedvalues, five_clst.taxi_call_num, 0, 745, \"may_2014\")\n",
    "plot_result(prediction(final_model_clust_5, components=72, feature_num=2, dataframe=five_clst), \n",
    "            best_model.fittedvalues, five_clst.taxi_call_num, 13920, 14664, \"december_2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### For all regions from cluster 5 we've got linear regression model on 60 week (t=168), 30 hour (t=1) and no annual fourier componentsС‚ + ARIMA(10, 1, 1, 1) for regression residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now we can use all selected parameters to fit training data and predict taxi calls in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = [[7, 11, 12, 13, 14, 15, 21, 27, 28, 31, 41, 61, 71, 72, 73, 84],\n",
    "            [74, 76, 77, 79, 80, 81, 82, 83, 85, 87, 88, 91],\n",
    "            [0, 3, 4, 5, 6, 17, 18, 19, 32, 33, 34, 35, 36, 44, 45, 95, 96, 97, 98, 99, 100, 101],\n",
    "            [8, 16, 24, 25, 26, 29, 30, 38, 39, 40, 51, 52, 62, 63, 64, 65, 75, 89, 90, 92],\n",
    "            [1, 2, 10, 20, 22, 23, 42, 43, 46, 47, 49, 50, 53, 54, 55, 56, 57, 58, 66, 67, 69, 78, 93, 94],\n",
    "            [9, 37, 48, 59, 60, 68, 70, 86],\n",
    "           ]\n",
    "\n",
    "model_parameters = [(((168, 70), (8760, 15), (1, 12)), (5, 1, 1, 1, 1, 0)),\n",
    "                    (((168, 70), (8760, 20), (1, 12)), (14, 1, 1, 1, 1, 0)),\n",
    "                    (((168, 60), (8760, 15), (11, 12)), (3, 1, 1, 1, 1, 0)),\n",
    "                    (((168, 60), (8760, 20), (1, 30)), (9, 2, 1, 1, 1, 0)),\n",
    "                    (((168, 60), (1, 30)), (11, 1, 1, 1, 1, 0)),\n",
    "                    (((168, 80), (8760, 15), (2, 23)), (13, 2, 1, 1, 1, 0)),\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get 2-year long fitted datafarme and frame with predictions: create empty frames - rows time, columns - regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_pred, c_red = verify_time.shape[0] + time.shape[0], data_2month.T[0, :].shape[0]\n",
    "\n",
    "t_full = pd.date_range('2014 May 1 00:00:00', periods = data_2year.shape[1] +  data_2month.shape[1] - 2, freq = 'h')\n",
    "f = lambda x: pd.DataFrame(np.zeros((r_pred, c_red)), index = [t_full], columns=[data_2month.T[0, :].astype(int)])\n",
    "\n",
    "pred_taxi_regress, pred_taxi_arima1, pred_taxi_arima2 = f(1), f(1), f(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fill three dataframes with regression values and arima values \"from past\" and arima prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pred = pd.date_range('2014-05-01 00:00:00', '2016-06-07 23:00:00', freq = 'h')\n",
    "\n",
    "for number, cluster in enumerate(clusters):\n",
    "    for region in cluster:\n",
    "        if region not in [7, 11, 12, 13, 14, 15, 21, 27, 28, 31, 41]:\n",
    "            frame = make_prediction(regres=model_parameters[number][0], arima=model_parameters[number][1], region=region, \n",
    "                                    prediction_period=('2016-05-01 00:00:00', '2016-06-07 23:00:00'), draw=False)\n",
    "            if isinstance(frame, pd.DataFrame):\n",
    "                pred_taxi_regress.iloc[:t_pred.shape[0], region] = frame.regression_prediction.values\n",
    "                pred_taxi_arima1.iloc[:t_pred.shape[0], region] = frame.average_resid_pred.values\n",
    "                pred_taxi_arima2.iloc[:t_pred.shape[0], region] = frame.resid_prediction.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_taxi_regress.round().to_csv(\"./data/week_4_regression\", sep='\\t', columns=list(pred_taxi_regress.columns))\n",
    "pred_taxi_arima1.round().to_csv(\"./data/week_4_arima_average\", sep='\\t', columns=list(pred_taxi_arima1.columns))\n",
    "pred_taxi_arima2.round().to_csv(\"./data/week_4_arima_predict\", sep='\\t', columns=list(pred_taxi_arima2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = lambda name: pd.read_csv(name, sep='\\t', index_col=\"Unnamed: 0\")\n",
    "\n",
    "reg_df, arima_av_df, arima_pred_df = k(\"./data/week_4_regression\"), \\\n",
    "                                     k(\"./data/week_4_arima_average\"), \\\n",
    "                                     k(\"./data/week_4_arima_predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now we may use r2_score from sklearn.metrics module to verify accuracy of our prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### plot one arbitrary selected month - let it be april 2015 and calculated r2 value for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reg in range(102):\n",
    "    ''' select only train data '''\n",
    "    fitted_value = reg_df.iloc[:17544, reg].values + arima_av_df.iloc[:17544, reg].values\n",
    "    prediction_real(fitted_value, taxi.iloc[:, reg].values, (336*24, 365*24), 'april 2015 region: '+reg_df.columns[reg], \n",
    "                    \"taxi calls\", (15,5), ('prediction', 'real values')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r2_train = [r2_score(reg_df.iloc[:17544, reg].values + arima_av_df.iloc[:17544, reg].values, \n",
    "                     taxi.iloc[:17544, reg].values) \n",
    "            for reg in range(102)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train r-2 score: \", r2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot predicted two weeks of may 2016 in two variants -  using arima prediction and \"arima from past\" - arima fitted values exact one year ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reg in range(102):\n",
    "    ''' select only train data '''\n",
    "    fitted_pred = reg_df.iloc[17544:17544+168*2, reg].values + arima_pred_df.iloc[17544:17544+168*2, reg].values\n",
    "    fitted_av = reg_df.iloc[17544:17544+168*2, reg].values + arima_av_df.iloc[17544:17544+168*2, reg].values\n",
    "    real = verify_taxi.iloc[0:168*2, reg].values\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(real, alpha = 0.7, label = 'real values')\n",
    "    plt.plot(fitted_pred, alpha=0.5, label = 'prediction', color=\"red\")\n",
    "    plt.plot(fitted_av, alpha=0.5, label = 'prediction average arima', color=\"green\")\n",
    "    plt.xlabel('time (hours)')\n",
    "    plt.ylabel(\"taxi calls\")\n",
    "    plt.title('first half may 2016 region: '+reg_df.columns[reg])\n",
    "    plt.legend()\n",
    "    plt.grid()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2 supplementary materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### try to choose correct number of clusters using special clustering method  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### http://nbviewer.jupyter.org/github/alexminnaar/time-series-classification-and-clustering/blob/master/Time%20Series%20Classification%20and%20Clustering.ipynb      <br> <br> https://github.com/alexminnaar/time-series-classification-and-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ts_cluster(object):\n",
    "    def __init__(self,num_clust=100):\n",
    "        '''\n",
    "        num_clust is the number of clusters for the k-means algorithm\n",
    "        assignments holds the assignments of data points (indices) to clusters\n",
    "        centroids holds the centroids of the clusters\n",
    "        '''\n",
    "        self.num_clust=num_clust\n",
    "        self.assignments={}\n",
    "        self.centroids=[]\n",
    "    \n",
    "    def compa_clust(self,s1,centroid,w):        \n",
    "        centroid_part = centroid[:,:5]\n",
    "        self.assign = pd.Series([[] for i in range(len(centroid))],index=np.arange(len(centroid)))\n",
    "\n",
    "        for ind,i in enumerate(s1):\n",
    "            min_dist=float('inf')\n",
    "            #closest_clust=None\n",
    "            for c_ind,j in enumerate(centroid_part):\n",
    "                    \n",
    "                if self.LB_Keogh(i,j,5)<min_dist:\n",
    "                    cur_dist=self.SpatioTemporalDis(i, j, w)\n",
    "                        \n",
    "                    if cur_dist<min_dist:\n",
    "                        min_dist=cur_dist\n",
    "                        closest_clust=c_ind\n",
    "            \n",
    "            \n",
    "            self.assign[closest_clust].append(ind)\n",
    "\n",
    "        print(self.assign)\n",
    "        #print s1[1]\n",
    "        self.s2 = pd.Series([[] for i in range(len(s1))],index=np.arange(len(s1)))\n",
    "        #print self.s2\n",
    "        for key in self.assign.index:\n",
    "            for k in self.assign[key]:\n",
    "                self.s2[k] = centroid[key,6:].tolist()\n",
    "                #print self.s2[k]\n",
    "        self.s2 = np.array(self.s2)\n",
    "        return self.s2\n",
    "        \n",
    "    \n",
    "    \n",
    "    def k_means_clust(self,data,num_iter,w,progress=True):\n",
    "        \n",
    "        '''\n",
    "        k-means clustering algorithm for time series data.  dynamic time warping Euclidean distance\n",
    "         used as default similarity measure. \n",
    "        '''\n",
    "        self.centroids=random.sample(list(data),self.num_clust)\n",
    "        #print(len(self.centroids))\n",
    "        for n in range(num_iter):\n",
    "            if progress:\n",
    "                print('iteration '+str(n+1), end=' ')\n",
    "            \n",
    "            self.assignments={}\n",
    "            for ind,i in enumerate(data):\n",
    "                min_dist=float('inf')\n",
    "                #closest_clust=None\n",
    "                for c_ind,j in enumerate(self.centroids):\n",
    "                    if self.LB_Keogh(i,j,5)<min_dist:\n",
    "                        cur_dist=self.SpatioTemporalDis(i, j, w)\n",
    "                        \n",
    "                        if cur_dist<min_dist:\n",
    "                            min_dist=cur_dist\n",
    "                            closest_clust=c_ind\n",
    "                \n",
    "                if closest_clust in self.assignments:\n",
    "                    self.assignments[closest_clust].append(ind)\n",
    "                else:\n",
    "                    self.assignments[closest_clust]=[]\n",
    "                    \n",
    "            #print(len(self.assignments))\n",
    "            #recalculate centroids of clusters\n",
    "            for key in self.assignments:\n",
    "                clust_sum=0\n",
    "                for k in self.assignments[key]:\n",
    "                    clust_sum=clust_sum+data[k]\n",
    "                self.centroids[key]=[m/len(self.assignments[key]) for m in clust_sum]\n",
    "            \n",
    "    def get_centroids(self):\n",
    "        return self.centroids\n",
    "        \n",
    "    def get_assignments(self):\n",
    "        return self.assignments\n",
    "        \n",
    "    def plot_centroids(self):\n",
    "        for i in self.centroids:\n",
    "            plt.plot(i)\n",
    "        plt.show()\n",
    "        \n",
    "    def SpatioTemporalDis(self,s1,s2,w):\n",
    "        \n",
    "        f1,f2 = s1[0:2],s2[0:2]\n",
    "        v1,v2 = s1[2:],s2[2:]\n",
    "        disInvari = self.InvarDistance(f1, f2)\n",
    "        disVari   = self.DTWDistance(v1, v2, w)\n",
    "        \n",
    "        return np.sqrt(disInvari+disVari)\n",
    "    \n",
    "    \n",
    "    def InvarDistance(self,f1,f2):\n",
    "        '''calculates the invariant features distance using Euclidean distance'''\n",
    "         \n",
    "        dis = spatial.distance.euclidean(f1, f2)\n",
    "        \n",
    "        return dis\n",
    "        \n",
    "    def DTWDistance(self,s1,s2,w=None):\n",
    "        '''\n",
    "        Calculates dynamic time warping Euclidean distance between two\n",
    "        sequences. Option to enforce locality constraint for window w.\n",
    "        '''\n",
    "        DTW={}\n",
    "        \n",
    "        if w:\n",
    "            w = max(w, abs(len(s1)-len(s2)))\n",
    "    \n",
    "            for i in range(-1,len(s1)):\n",
    "                for j in range(-1,len(s2)):\n",
    "                    DTW[(i, j)] = float('inf')\n",
    "            \n",
    "        else:\n",
    "            for i in range(len(s1)):\n",
    "                DTW[(i, -1)] = float('inf')\n",
    "            for i in range(len(s2)):\n",
    "                DTW[(-1, i)] = float('inf')\n",
    "        \n",
    "        DTW[(-1, -1)] = 0\n",
    "        \n",
    "        for i in range(len(s1)):\n",
    "            if w:\n",
    "                for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "                    dist= (s1[i]-s2[j])**2\n",
    "                    DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "            else:\n",
    "                for j in range(len(s2)):\n",
    "                    dist= (s1[i]-s2[j])**2\n",
    "                    DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "            \n",
    "        return (DTW[len(s1)-1, len(s2)-1])      \n",
    "        \n",
    "    def LB_Keogh(self,s1,s2,r):\n",
    "        '''\n",
    "        Calculates LB_Keough lower bound to dynamic time warping. Linear\n",
    "        complexity compared to quadratic complexity of dtw.\n",
    "        '''\n",
    "        LB_sum=0\n",
    "        for ind,i in enumerate(s1):\n",
    "            \n",
    "            lower_bound=min(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "            upper_bound=max(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "            \n",
    "            if i>upper_bound:\n",
    "                LB_sum=LB_sum+(i-upper_bound)**2\n",
    "            elif i<lower_bound:\n",
    "                LB_sum=LB_sum+(i-lower_bound)**2\n",
    "            \n",
    "        return np.sqrt(LB_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### may choose number of clusters, number of iterations and size of window w: trying different values - lets take number of iterations = 20, window size = 5 (default value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_obj = ts_cluster(num_clust=4)\n",
    "clust_obj.k_means_clust(taxi_data.T.values[:, :744],num_iter=20,w=5,progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clust_obj.get_centroids()\n",
    "#clust_obj.get_assignments()\n",
    "#list(clust_obj.get_assignments().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_centr_label(clust_obj, num_clust):\n",
    "    cluster_centroids = clust_obj.get_centroids()\n",
    "    cluster_labels = np.array([num_clust]*102)\n",
    "    for i in range(cluster_labels.shape[0]):\n",
    "        label = [key for key, values in clust_obj.get_assignments().items() if i in values]\n",
    "        if len(label) > 0:\n",
    "            cluster_labels[i] = label[0]\n",
    "    return cluster_centroids, cluster_labels       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "four_clust_centroids, four_clust_labels = get_centr_label(clust_obj_more, num_clust=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_clust_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_obj.plot_centroids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_centroid(clust_obj, plt_num=4):\n",
    "    f, axarr = plt.subplots(int(np.ceil(plt_num / 3.0)), 3)\n",
    "    f.set_size_inches(15, 10)\n",
    "    for i in range(plt_num):\n",
    "        axarr[i//3, i%3].plot(clust_obj.get_centroids()[i])\n",
    "        axarr[i//3, i%3].set_title('cluster: ' + str(i))\n",
    "    f.subplots_adjust(hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_centroid(clust_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouette_coeff(num_clusters=4, cluster_labels=four_clust_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### try to choose different cluster numbers from 5 to 10 - do it for first month of time serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.T.values[:, :744].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = []\n",
    "for clst_num in range(5, 11):\n",
    "    clusters.append(ts_cluster(num_clust=clst_num))\n",
    "    clusters[-1].k_means_clust(taxi_data.T.values[:, :744],num_iter=20,w=5,progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for clst_num, clst_obj in enumerate(clusters):\n",
    "    _, clust_labels = get_centr_label(clst_obj, num_clust=5+clst_num)\n",
    "    plot_silhouette_coeff(num_clusters=5+clst_num, cluster_labels=clust_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On my opinion n_clusters = 6 looks the best. Clusterization method use not euclidean metrics but silouette coeff calculation utilize it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This way number of clusters equal to 6 was choosen\n",
    "##### for each cluster print \"bad\" series with negative silouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_centroids, clust_labels = get_centr_label(clusters[1], num_clust=5+clst_num)\n",
    "bad_elements = plot_silhouette_coeff(6, clust_labels)\n",
    "print(bad_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at each cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_centroid(clusters[1], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[1].get_assignments().values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot all time series. \"good\"regions are plotted in blue color (\"+\" silouette coeffisient), \"bad\" region with negative coefficient are plotted in green color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for clst_num, cluster in enumerate(list(clusters[1].get_assignments().values())):\n",
    "    f, axarr = plt.subplots(int(np.ceil(len(cluster) / 3.0)), 3)\n",
    "    f.set_size_inches(15, 5*int(np.ceil(len(cluster) / 3.0)))\n",
    "    for i in range(len(cluster)):\n",
    "        if cluster[i] in bad_elements[clst_num]:\n",
    "            axarr[i//3, i%3].plot(taxi_data.T.values[cluster[i], :744], color=\"green\")\n",
    "        else:\n",
    "            axarr[i//3, i%3].plot(taxi_data.T.values[cluster[i], :744])\n",
    "        axarr[i//3, i%3].set_title('cluster: ' + str(clst_num) + \" region index: \" + str(cluster[i]))\n",
    "    f.subplots_adjust(hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(itertools.chain.from_iterable(clusters[1].get_assignments().values()))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
